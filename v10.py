import os
import csv
import pandas as pd
from typing import List, Dict, Tuple
from sentence_transformers import CrossEncoder
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI
from rag_reasoning import (ReasoningDataQueryEngine,ReasoningDataQuery)
# from reasoning import (ReasoningDataQueryEngine,ReasoningDataQuery)
from dotenv import load_dotenv

DEBUG = True
FETCH_K = 20
TOP_K = 5 
CONFIDENCE_THRESHOLD = 1.8

def load_csv(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    df.columns = [c.strip() for c in df.columns]
    df = df.fillna("")
    df["Gi√° (VNƒê)"] = df["Gi√° (VNƒê)"].apply(lambda x: int(x) if str(x).strip() != "" else 0)
    df["Chu k·ª≥ (ng√†y)"] = df["Chu k·ª≥ (ng√†y)"].apply(lambda x: int(x) if str(x).strip() != "" else 0)
    return df

def df_to_documents(df: pd.DataFrame, source: str) -> List[Document]:
    docs = []
    for _, row in df.iterrows():
        code = str(row.get("M√£ d·ªãch v·ª•", "")).strip()
        text_parts = []
        for col in df.columns:
            val = row[col]
            text_parts.append(f"{col}: {val}")
        text = " . ".join(text_parts)
        docs.append(Document(page_content=text, metadata={"source": source, "service_code": code}))
    return docs



def chunk_documents(docs: List[Document], chunk_size=512, chunk_overlap=50):
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap
    )
    return splitter.split_documents(docs)



def build_vectorstore(docs: List[Document], persist_dir="chroma_db"):
    emb = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
    vect = Chroma.from_documents(
        docs,
        embedding=emb,
        ids = [f"{d.metadata.get('service_code','unknown')}_{i}" for i, d in enumerate(docs)],
        persist_directory=persist_dir,
    )
    return vect


def make_reranked_retriever(vectorstore, fetch_k=30, top_k=5):
    # retriever = vectorstore.as_retriever(search_kwargs={"k": fetch_k})
    retriever = vectorstore.as_retriever(search_kwargs={"k": top_k})
    reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

    
    def retrieve_with_scores(question: str) -> List[Tuple[Document, float]]:
        # get candidates (list of Document)
        # rq = reasoning_engine.compile(question)
        # if DEBUG:
        #     print(f"[DEBUG] question Compiled: {rq}")

        candidates = retriever.invoke(question)
        if DEBUG:
            print(f"[DEBUG] raw candidates count: {len(candidates)}")
        # dedupe by service_code (keep first occurrence)

        # uniq = []
        # seen = set()
        # for d in candidates:
        #     code = d.metadata.get("service_code")
        #     if code not in seen:
        #         uniq.append(d)
        #         seen.add(code)
        # candidates = uniq
        # if DEBUG:
        #     print(f"[DEBUG] deduped candidates count: {len(candidates)}")
        #     for d in candidates[:10]:
        #         meta = d.metadata
        #         print("[CAND]", meta, "->", d.page_content[:120].replace("\n"," "))
        # if not candidates:
        #     return []

        pairs = [(question, d.page_content) for d in candidates]
        scores = reranker.predict(pairs)  # array of floats
        scored = list(zip(candidates, [float(s) for s in scores]))
        scored_sorted = sorted(scored, key=lambda x: x[1], reverse=True)
        if DEBUG:
            print("[DEBUG] top scored (doc, score):")
            for doc, sc in scored_sorted[:min(10, len(scored_sorted))]:
                print(f"  {doc.metadata.get('service_code')} -> {sc:.4f}")
        return scored_sorted[:top_k]


    return retrieve_with_scores





RAG_PROMPT = """
üéØ Vai tr√≤:
B·∫°n l√† m·ªôt **tr·ª£ l√Ω ·∫£o th√¥ng minh** c√≥ nhi·ªám v·ª• h·ªó tr·ª£ kh√°ch h√†ng v·ªÅ **c√°c g√≥i c∆∞·ªõc c·ªßa nh√† m·∫°ng Viettel**.

---

üß© Nhi·ªám v·ª•:
B·∫°n c·∫ßn **tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ch·ªâ d·ª±a tr√™n d·ªØ li·ªáu ƒë√£ cung c·∫•p trong "Ng·ªØ c·∫£nh"**.  
Ng·ªØ c·∫£nh l√† c∆° s·ªü d·ªØ li·ªáu d·∫°ng b·∫£ng, m·ªói h√†ng t∆∞∆°ng ·ª©ng v·ªõi m·ªôt g√≥i c∆∞·ªõc, c√°c c·ªôt m√¥ t·∫£ thu·ªôc t√≠nh c·ª• th·ªÉ.

C√°c c·ªôt c·ªßa b·∫£ng bao g·ªìm:
> M√£ d·ªãch v·ª•, Th·ªùi gian thanh to√°n, C√°c d·ªãch v·ª• ti√™n quy·∫øt, Gi√° (VNƒê), Chu k·ª≥ (ng√†y),
> 4G t·ªëc ƒë·ªô ti√™u chu·∫©n/ng√†y, 4G t·ªëc ƒë·ªô cao/ng√†y, 4G t·ªëc ƒë·ªô ti√™u chu·∫©n/chu k·ª≥, 4G t·ªëc ƒë·ªô cao/chu k·ª≥,
> G·ªçi n·ªôi m·∫°ng, G·ªçi ngo·∫°i m·∫°ng, Tin nh·∫Øn, Chi ti·∫øt, T·ª± ƒë·ªông gia h·∫°n, C√∫ ph√°p ƒëƒÉng k√Ω.

M·ªôt s·ªë √¥ c√≥ th·ªÉ tr·ªëng (t√πy g√≥i c∆∞·ªõc).

---

üß† Ghi nh·ªõ quy t·∫Øc:
1. N·∫øu d·ªØ li·ªáu "4G t·ªëc ƒë·ªô cao/ng√†y" ho·∫∑c "4G t·ªëc ƒë·ªô ti√™u chu·∫©n/ng√†y" l√† s·ªë d∆∞∆°ng  
   ‚Üí Nghƒ©a l√† ng∆∞·ªùi d√πng ƒë∆∞·ª£c s·ª≠ d·ª•ng t·ªëi ƒëa l∆∞·ª£ng d·ªØ li·ªáu ƒë√≥ m·ªói ng√†y, sau ƒë√≥ reset v√†o ng√†y ti·∫øp theo.

2. N·∫øu **kh√¥ng c√≥ d·ªØ li·ªáu theo ng√†y**, nh∆∞ng c√≥ d·ªØ li·ªáu theo chu k·ª≥  
   ‚Üí Nghƒ©a l√† to√†n b·ªô dung l∆∞·ª£ng ƒë√≥ d√πng chung cho to√†n chu k·ª≥.

3. Khi ng∆∞·ªùi d√πng h·ªèi v·ªÅ **dung l∆∞·ª£ng data**, h√£y tra c·ª©u c√°c c·ªôt sau:
   - "4G t·ªëc ƒë·ªô ti√™u chu·∫©n/ng√†y"
   - "4G t·ªëc ƒë·ªô cao/ng√†y"
   - "4G t·ªëc ƒë·ªô ti√™u chu·∫©n/chu k·ª≥"
   - "4G t·ªëc ƒë·ªô cao/chu k·ª≥"
   - "Chi ti·∫øt"

4. Trong **m·ªçi c√¢u tr·∫£ l·ªùi**, b·∫°n ph·∫£i tr√≠ch d·∫´n t·ªëi thi·ªÉu c√°c c·ªôt:
   - "M√£ d·ªãch v·ª•"
   - "C√∫ ph√°p ƒëƒÉng k√Ω"
   - "Gi√° (VNƒê)"
   - "Chi ti·∫øt"

5. N·∫øu ng∆∞·ªùi d√πng n√≥i ‚Äúƒëi·ªán tho·∫°i c·ª•c g·∫°ch‚Äù, ‚Äúnghe g·ªçi √≠t‚Äù, ho·∫∑c ‚Äú√≠t d√πng m·∫°ng‚Äù  
   ‚Üí Hi·ªÉu l√† c·∫ßn **g·ª£i √Ω g√≥i c∆∞·ªõc r·∫ª nh·∫•t** (tra c·ªôt ‚ÄúGi√° (VNƒê)‚Äù ƒë·ªÉ ch·ªçn gi√° nh·ªè nh·∫•t).

6. N·∫øu ng∆∞·ªùi d√πng h·ªèi **g√≥i n√†o r·∫ª h∆°n**,  
   ‚Üí So s√°nh c·ªôt ‚ÄúGi√° (VNƒê)‚Äù gi·ªØa c√°c g√≥i.

7. N·∫øu ng∆∞·ªùi d√πng h·ªèi **g√≥i n√†o r·∫ª nh·∫•t**,  
   ‚Üí Ch·ªçn g√≥i c√≥ gi√° tr·ªã **MIN c·ªßa c·ªôt ‚ÄúGi√° (VNƒê)‚Äù**.

8. N·∫øu c√≥ **nhi·ªÅu b·∫£n ghi tr√πng l·∫∑p**,  
   ‚Üí Ch·ªâ c·∫ßn t·ªïng h·ª£p v√† **tr·∫£ l·ªùi t√≥m t·∫Øt n·ªôi dung ch√≠nh m·ªôt l·∫ßn**.

---

üö´ N·∫øu kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi r√µ r√†ng trong c∆° s·ªü d·ªØ li·ªáu, **h√£y tr·∫£ l·ªùi ch√≠nh x√°c**:
"T√¥i kh√¥ng bi·∫øt - vui l√≤ng li√™n h·ªá t·ªïng ƒë√†i 18001090 ho·∫∑c email support@telco.vn"

---

üó£Ô∏è Y√™u c·∫ßu ƒë·ªãnh d·∫°ng c√¢u tr·∫£ l·ªùi:
- Vi·∫øt **ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu cho ng∆∞·ªùi d√πng ph·ªï th√¥ng**.
- Gi·ªØ nguy√™n **ng√¥n ng·ªØ c·ªßa ng∆∞·ªùi d√πng** (∆∞u ti√™n ti·∫øng Vi·ªát).
- Khi tr√≠ch d·∫´n, **lu√¥n ghi r√µ ngu·ªìn theo d·∫°ng [source: filename]**.
- **Tuy·ªát ƒë·ªëi kh√¥ng suy lu·∫≠n ho·∫∑c b·ªãa th√¥ng tin** kh√¥ng c√≥ trong d·ªØ li·ªáu.

---

Ng·ªØ c·∫£nh:
{context}

C√¢u h·ªèi ng∆∞·ªùi d√πng:
{question}

Tr·∫£ l·ªùi:
"""

prompt_template = ChatPromptTemplate.from_template(RAG_PROMPT)



def make_qa_chain(vectorstore, model_name:str="gemini-2.0-flash", temperature:float=0.0, fetch_k:int=FETCH_K, top_k:int=TOP_K, confidence_threshold:float=CONFIDENCE_THRESHOLD):
    llm = ChatGoogleGenerativeAI(model=model_name, temperature=temperature)
    retriever_with_scores = make_reranked_retriever(vectorstore, fetch_k=fetch_k, top_k=top_k)

    def answer_query(question: str):
        scored = retriever_with_scores(question)  # List[(Document, score)]
        if not scored:
            if DEBUG: print("[DEBUG] No candidates -> fallback")
            return {"answer": "T√¥i kh√¥ng bi·∫øt - vui l√≤ng li√™n h·ªá t·ªïng ƒë√†i 18001090 ho·∫∑c email support@telco.vn", "docs": [], "hallucinated": False}

        docs, scores = zip(*scored)
        # Confidence check: use top score (or average) to decide
        top_score = float(scores[0])
        avg_score = float(sum(scores)/len(scores))
        if DEBUG:
            print(f"[DEBUG] top_score={top_score:.4f}, avg_score={avg_score:.4f}, threshold={confidence_threshold}")

        if top_score < confidence_threshold:
            if DEBUG: print("[DEBUG] top score below threshold -> fallback")
            return {"answer": "T√¥i kh√¥ng bi·∫øt - vui l√≤ng li√™n h·ªá t·ªïng ƒë√†i 18001090 ho·∫∑c email support@telco.vn", "docs": [d.metadata for d in docs], "hallucinated": False}

        # Build the context: include service_code and page_content for each doc
        context_sections = []
        for d in docs:
            src = d.metadata.get("source", "unknown")
            code = d.metadata.get("service_code", "")
            context_sections.append(f"[source: {src} | service_code: {code}]\n{d.page_content}")
        context = "\n\n".join(context_sections)

        # Build prompt and invoke LLM
        prompt = RAG_PROMPT.format(context=context, question=question)
        if DEBUG:
            print("=== Prompt to LLM ===")
            print(prompt[:2000])  # only print head
            print("=== End Prompt ===")
        out = llm.invoke(prompt)
        # ChatGoogleGenerativeAI returns an object; try to extract string
        answer = out.content.strip() if hasattr(out, "content") else str(out).strip()

        # Determine hallucination: ensure at least one [source:] present
        hallucinated = "[source:" not in answer
        return {"answer": answer, "docs": [d.metadata for d in docs], "hallucinated": hallucinated}

    return answer_query



if __name__ == "__main__":
    load_dotenv()
    api_key = os.getenv("GOOGLE_API_KEY")


    df = load_csv("viettel.csv")
    docs = df_to_documents(df, "viettel.csv")
    chunks = chunk_documents(docs)
    vect = build_vectorstore(chunks)

    qa = make_qa_chain(vect, model_name="gemini-2.0-flash", temperature=0.0, fetch_k=FETCH_K, top_k=TOP_K, confidence_threshold=CONFIDENCE_THRESHOLD)


    queries = [
        "ƒê·ªÉ ƒëƒÉng k√Ω g√≥i MXH120 th√¨ so·∫°n tin g·ª≠i 191 ƒë√∫ng kh√¥ng?",
        "N√™u c√∫ ph√°p ƒëƒÉng k√Ω c·ªßa g√≥i c∆∞·ªõc c√≥ gi√° 120000 VNƒê v√† c√≥ ∆∞u ƒë√£i mi·ªÖn ph√≠ data",
        "Li·ªát k√™ t·∫•t c·∫£ c√°c g√≥i c∆∞·ªõc c√≥ chu k·ª≥ l·ªõn h∆°n 30 ng√†y?"
    ]
    for q in queries:
        print("\n---\nQUESTION:", q)
        res = qa(q)
        print("ANSWER:", res["answer"])
        print("HALLUCINATED:", res["hallucinated"])
        print("SOURCES:", res["docs"])