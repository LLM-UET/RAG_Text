import os
from typing import List, Dict
import csv, glob
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI


from sentence_transformers import CrossEncoder
from dotenv import load_dotenv

def load_service_csv(path: str):
    docs = []
    with open(path, newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            def get(k): return row.get(k, "").strip()
            code = get("MÃ£ dá»‹ch vá»¥")
            text = (
                f"GÃ³i cÆ°á»›c {code} ({get('Thá»i gian thanh toÃ¡n')}). "
                f"GiÃ¡: {get('GiÃ¡ (VNÄ)')}Ä‘ / {get('Chu ká»³ (ngÃ y)')} ngÃ y. "
                f"4G tá»‘c Ä‘á»™ cao/ngÃ y: {get('4G tá»‘c Ä‘á»™ cao/ngÃ y')}. "
                f"Tá»± Ä‘á»™ng gia háº¡n: {get('Tá»± Ä‘á»™ng gia háº¡n')}. "
                f"CÃº phÃ¡p Ä‘Äƒng kÃ½: {get('CÃº phÃ¡p Ä‘Äƒng kÃ½')}. "
            )
            if get('Chi tiáº¿t'): text += f"Chi tiáº¿t: {get('Chi tiáº¿t')}. "
            if get('Gá»i ná»™i máº¡ng'): text += f"Gá»i ná»™i máº¡ng: {get('Gá»i ná»™i máº¡ng')}. "
            if get('Gá»i ngoáº¡i máº¡ng'): text += f"Gá»i ngoáº¡i máº¡ng: {get('Gá»i ngoáº¡i máº¡ng')}. "
            docs.append(Document(page_content=text, metadata={"source": path, "service_code": code}))
    # print("-"*10)
    # print("Docs:", docs)
    # print("-"*10)
    return docs

def chunk_documents(docs: List[Document], chunk_size=1024, chunk_overlap=100):
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap
    )
    return splitter.split_documents(docs)

def build_vectorstore(docs: List[Document], persist_dir: str="chroma_db", use_openai=False):
    if use_openai:
        emb = OpenAIEmbeddings()
    else:
        emb = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
    vect = Chroma.from_documents(documents=docs, embedding=emb, ids=[d.metadata["service_code"] + f"_{i}" for i, d in enumerate(docs)], persist_directory=persist_dir)
    # print(type(vect))
    # vect.persist()
    return vect

def make_reranked_retriever(vectorstore, fetch_k=20, top_k=5):
    """
    1. Láº¥y trÆ°á»›c fetch_k candidates tá»« retriever gá»‘c
    2. Rerank báº±ng cross-encoder (máº¡nh hÆ¡n cosine similarity)
    3. Tráº£ vá» top_k tá»‘t nháº¥t
    """
    retriever = vectorstore.as_retriever(search_kwargs={"k": fetch_k})
    reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
    # reranker = CrossEncoder("cross-encoder/ms-marco-electra-base", max_length=512)

    def retrieve(question: str) -> List[Document]:
        candidates = retriever.invoke(question)
        # print(candidates)
        unique_candidates = []
        seen = set()
        for d in candidates:
            code = d.metadata.get("service_code")
            if code not in seen:
                unique_candidates.append(d)
                seen.add(code)
        candidates = unique_candidates
        if not candidates:
            return []
        pairs = [(question, d.page_content) for d in candidates]
        # print(pairs)
        scores = reranker.predict(pairs)
        ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
        # print(ranked)
        reranked_docs = [doc for doc, _ in ranked[:top_k]]
        # print(reranked_docs)
        # for d in candidates:
        #     print("CAND:", d.metadata, "=>", d.page_content[:80])
        return reranked_docs

    return retrieve


# def make_reranked_retriever(vectorstore, fetch_k=20, top_k=5):
#     """
#     1. Láº¥y trÆ°á»›c fetch_k candidates tá»« retriever gá»‘c
#     2. Rerank báº±ng cross-encoder (máº¡nh hÆ¡n cosine similarity)
#     3. Tráº£ vá» top_k tá»‘t nháº¥t
#     """
#     retriever = vectorstore.as_retriever(search_kwargs={"fetch_k": fetch_k, "k": top_k})
#     reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

#     def retrieve(question: str) -> List[Document]:
#         candidates = retriever.invoke(question)
#         if not candidates:
#             return []
#         pairs = [(question, d.page_content) for d in candidates]
#         scores = reranker.predict(pairs)
#         ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
#         reranked_docs = [doc for doc, _ in ranked]
#         return reranked_docs

#     return retrieve


# RAG_PROMPT = """You are a helpful customer support assistant for a telecom operator.

# Answer the user's question using only the provided context.
# If the context doesn't contain the answer, reply exactly:
# "I don't know - please contact support at 18001090 or support@telco.vn"

# Always cite your sources as [source: filename].

# Context:
# {context}

# User question:
# {question}

# Answer concisely in Vietnamese (or match user's language).
# """

RAG_PROMPT = """
ğŸ¯ Vai trÃ²:
Báº¡n lÃ  má»™t **trá»£ lÃ½ áº£o thÃ´ng minh** cÃ³ nhiá»‡m vá»¥ há»— trá»£ khÃ¡ch hÃ ng vá» **cÃ¡c gÃ³i cÆ°á»›c cá»§a nhÃ  máº¡ng Viettel**.

---

ğŸ§© Nhiá»‡m vá»¥:
Báº¡n cáº§n **tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng chá»‰ dá»±a trÃªn dá»¯ liá»‡u Ä‘Ã£ cung cáº¥p trong "Ngá»¯ cáº£nh"**.  
Ngá»¯ cáº£nh lÃ  cÆ¡ sá»Ÿ dá»¯ liá»‡u dáº¡ng báº£ng, má»—i hÃ ng tÆ°Æ¡ng á»©ng vá»›i má»™t gÃ³i cÆ°á»›c, cÃ¡c cá»™t mÃ´ táº£ thuá»™c tÃ­nh cá»¥ thá»ƒ.

CÃ¡c cá»™t cá»§a báº£ng bao gá»“m:
> MÃ£ dá»‹ch vá»¥, Thá»i gian thanh toÃ¡n, CÃ¡c dá»‹ch vá»¥ tiÃªn quyáº¿t, GiÃ¡ (VNÄ), Chu ká»³ (ngÃ y),
> 4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/ngÃ y, 4G tá»‘c Ä‘á»™ cao/ngÃ y, 4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/chu ká»³, 4G tá»‘c Ä‘á»™ cao/chu ká»³,
> Gá»i ná»™i máº¡ng, Gá»i ngoáº¡i máº¡ng, Tin nháº¯n, Chi tiáº¿t, Tá»± Ä‘á»™ng gia háº¡n, CÃº phÃ¡p Ä‘Äƒng kÃ½.

Má»™t sá»‘ Ã´ cÃ³ thá»ƒ trá»‘ng (tÃ¹y gÃ³i cÆ°á»›c).

---

ğŸ§  Ghi nhá»› quy táº¯c:
1. Náº¿u dá»¯ liá»‡u "4G tá»‘c Ä‘á»™ cao/ngÃ y" hoáº·c "4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/ngÃ y" lÃ  sá»‘ dÆ°Æ¡ng  
   â†’ NghÄ©a lÃ  ngÆ°á»i dÃ¹ng Ä‘Æ°á»£c sá»­ dá»¥ng tá»‘i Ä‘a lÆ°á»£ng dá»¯ liá»‡u Ä‘Ã³ má»—i ngÃ y, sau Ä‘Ã³ reset vÃ o ngÃ y tiáº¿p theo.

2. Náº¿u **khÃ´ng cÃ³ dá»¯ liá»‡u theo ngÃ y**, nhÆ°ng cÃ³ dá»¯ liá»‡u theo chu ká»³  
   â†’ NghÄ©a lÃ  toÃ n bá»™ dung lÆ°á»£ng Ä‘Ã³ dÃ¹ng chung cho toÃ n chu ká»³.

3. Khi ngÆ°á»i dÃ¹ng há»i vá» **dung lÆ°á»£ng data**, hÃ£y tra cá»©u cÃ¡c cá»™t sau:
   - "4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/ngÃ y"
   - "4G tá»‘c Ä‘á»™ cao/ngÃ y"
   - "4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/chu ká»³"
   - "4G tá»‘c Ä‘á»™ cao/chu ká»³"
   - "Chi tiáº¿t"

4. Trong **má»i cÃ¢u tráº£ lá»i**, báº¡n pháº£i trÃ­ch dáº«n tá»‘i thiá»ƒu cÃ¡c cá»™t:
   - "MÃ£ dá»‹ch vá»¥"
   - "CÃº phÃ¡p Ä‘Äƒng kÃ½"
   - "GiÃ¡ (VNÄ)"
   - "Chi tiáº¿t"

5. Náº¿u ngÆ°á»i dÃ¹ng nÃ³i â€œÄ‘iá»‡n thoáº¡i cá»¥c gáº¡châ€, â€œnghe gá»i Ã­tâ€, hoáº·c â€œÃ­t dÃ¹ng máº¡ngâ€  
   â†’ Hiá»ƒu lÃ  cáº§n **gá»£i Ã½ gÃ³i cÆ°á»›c ráº» nháº¥t** (tra cá»™t â€œGiÃ¡ (VNÄ)â€ Ä‘á»ƒ chá»n giÃ¡ nhá» nháº¥t).

6. Náº¿u ngÆ°á»i dÃ¹ng há»i **gÃ³i nÃ o ráº» hÆ¡n**,  
   â†’ So sÃ¡nh cá»™t â€œGiÃ¡ (VNÄ)â€ giá»¯a cÃ¡c gÃ³i.

7. Náº¿u ngÆ°á»i dÃ¹ng há»i **gÃ³i nÃ o ráº» nháº¥t**,  
   â†’ Chá»n gÃ³i cÃ³ giÃ¡ trá»‹ **MIN cá»§a cá»™t â€œGiÃ¡ (VNÄ)â€**.

8. Náº¿u cÃ³ **nhiá»u báº£n ghi trÃ¹ng láº·p**,  
   â†’ Chá»‰ cáº§n tá»•ng há»£p vÃ  **tráº£ lá»i tÃ³m táº¯t ná»™i dung chÃ­nh má»™t láº§n**.

---

ğŸš« Náº¿u khÃ´ng tÃ¬m tháº¥y cÃ¢u tráº£ lá»i rÃµ rÃ ng trong cÆ¡ sá»Ÿ dá»¯ liá»‡u, **hÃ£y tráº£ lá»i chÃ­nh xÃ¡c**:
"TÃ´i khÃ´ng biáº¿t - vui lÃ²ng liÃªn há»‡ tá»•ng Ä‘Ã i 18001090 hoáº·c email support@telco.vn"

---

ğŸ—£ï¸ YÃªu cáº§u Ä‘á»‹nh dáº¡ng cÃ¢u tráº£ lá»i:
- Viáº¿t **ngáº¯n gá»n, dá»… hiá»ƒu cho ngÆ°á»i dÃ¹ng phá»• thÃ´ng**.
- Giá»¯ nguyÃªn **ngÃ´n ngá»¯ cá»§a ngÆ°á»i dÃ¹ng** (Æ°u tiÃªn tiáº¿ng Viá»‡t).
- Khi trÃ­ch dáº«n, **luÃ´n ghi rÃµ nguá»“n theo dáº¡ng [source: filename]**.
- **Tuyá»‡t Ä‘á»‘i khÃ´ng suy luáº­n hoáº·c bá»‹a thÃ´ng tin** khÃ´ng cÃ³ trong dá»¯ liá»‡u.

---

Ngá»¯ cáº£nh:
{context}

CÃ¢u há»i ngÆ°á»i dÃ¹ng:
{question}

Tráº£ lá»i:
"""


prompt_template = ChatPromptTemplate.from_template(RAG_PROMPT)
# print(f"PROMPT_TEMPLATE: {RAG_PROMPT}")

def make_qa_chain(vectorstore, model_name="gemini-2.0-flash", temperature=0.0, use_openai_llm=True):
    if use_openai_llm:
        llm = ChatGoogleGenerativeAI(model=model_name, temperature=temperature)
    else:
        raise NotImplementedError("Only OpenAI LLM supported here")

    retriever_fn = make_reranked_retriever(vectorstore)

    rag_chain_lcel = (
        {
            "context": retriever_fn,
            "question": RunnablePassthrough()
        }
        | prompt_template
        | llm
        | StrOutputParser()
    )

    def answer_query(question: str, k:int=5):
        docs = retriever_fn(question)
        # context = "\n\n".join(
        #     [f"[source: {os.path.basename(d.metadata.get('source',''))}]\n{d.page_content}" for d in docs]
        # )
        # prompt = RAG_PROMPT.format(context=context, question=question)
        # out = llm.invoke(prompt)
        # answer = out.content.strip() if hasattr(out, "content") else str(out)

        # Build a plain-text debug prompt from the same template used by the chain
        # and print it so we can inspect what is being passed to the LLM.
        # try:
        #     prompt = RAG_PROMPT.format(context=context, question=question)
        # except Exception:
        #     # Fallback: if formatting fails for any reason, create a minimal prompt
        #     prompt = f"Context:\n{context}\n\nUser question:\n{question}\n"

        # print("=== Prompt sent to LLM ===")
        # print(prompt)
        # print("=== End Prompt ===")

        # Invoke the composed chain (it will construct prompts/messages internally too)
        answer = rag_chain_lcel.invoke(question)

        hallucinated = "[source:" not in answer
        return {
            "answer": answer,
            "docs": docs,
            "hallucinated": hallucinated
        }

    return answer_query

if __name__ == "__main__":
    load_dotenv()
    api_key = os.getenv("GOOGLE_API_KEY")
    docs = load_service_csv("viettel.csv")
    chunks_docs = chunk_documents(docs)
    # print(docs)
    vect = build_vectorstore(chunks_docs)
    # print(vect)
    qa = make_qa_chain(vect)
    # print(qa)

    # q = "GÃ³i MXH120 cÃ³ pháº£i lÃ  gÃ³i tráº£ trÆ°á»›c khÃ´ng?"
    # q="GÃ³i MXH100 cÃ³ tá»± Ä‘á»™ng gia háº¡n khÃ´ng?"
    # q="GÃ³i V120B cÃ³ Ä‘Æ°á»£c miá»…n phÃ­ gá»i ná»™i máº¡ng khÃ´ng?"
    # q="GÃ³i MXH120 cÃ³ ná»™i dung gÃ¬?"
    # q = "GiÃ¡ cá»§a gÃ³i cÆ°á»›c MXH100 lÃ  bao nhiÃªu?"
    # q = "Tin nháº¯n cá»§a gÃ³i VB90 lÃ  gÃ¬?"
    # q = "Äá»ƒ Ä‘Äƒng kÃ½ gÃ³i MXH120 thÃ¬ soáº¡n tin gá»­i 191 Ä‘Ãºng khÃ´ng?"
    # q = "Äá»ƒ Ä‘Äƒng kÃ½ gÃ³i MXH100 thÃ¬ soáº¡n tin gá»­i 290 Ä‘Ãºng khÃ´ng?"
    # q = "Liá»‡t kÃª cÃ¡c gÃ³i cÆ°á»›c miá»…n phÃ­ data."
    # q = "NÃªu cÃº phÃ¡p Ä‘Äƒng kÃ½ cá»§a gÃ³i cÆ°á»›c cÃ³ giÃ¡ 120.000 VNÄ cÃ³ Æ°u Ä‘Ã£i miá»…n phÃ­ gá»i ná»™i máº¡ng"
    q = "NÃªu cÃº phÃ¡p Ä‘Äƒng kÃ½ cá»§a gÃ³i cÆ°á»›c cÃ³ giÃ¡ 120.000 VNÄ vÃ  cÃ³ Æ°u Ä‘Ã£i miá»…n phÃ­ data"
    result = qa(q)

    print("QUESTION", q)
    print("ANSWER:", result["answer"])
    print("HALLUCINATED:", result["hallucinated"])
    print("SOURCES:", [d.metadata for d in result["docs"]])


    # q_array = ["GÃ³i MXH120 cÃ³ pháº£i lÃ  gÃ³i tráº£ trÆ°á»›c khÃ´ng?", "GÃ³i MXH100 cÃ³ tá»± Ä‘á»™ng gia háº¡n khÃ´ng?", "GÃ³i V120B cÃ³ Ä‘Æ°á»£c miá»…n phÃ­ gá»i ná»™i máº¡ng khÃ´ng?", "GÃ³i MXH120 cÃ³ ná»™i dung gÃ¬?", "GiÃ¡ cá»§a gÃ³i cÆ°á»›c MXH100 lÃ  bao nhiÃªu?", "Tin nháº¯n cá»§a gÃ³i VB90 lÃ  gÃ¬?", "Äá»ƒ Ä‘Äƒng kÃ½ gÃ³i MXH120 thÃ¬ soáº¡n tin gá»­i 191 Ä‘Ãºng khÃ´ng?", "Äá»ƒ Ä‘Äƒng kÃ½ gÃ³i MXH100 thÃ¬ soáº¡n tin gá»­i 290 Ä‘Ãºng khÃ´ng?"]
    
    # q_array = ["GÃ³i cÆ°á»›c SD70 cÃ³ giÃ¡ bao nhiÃªu vÃ  cung cáº¥p bao nhiÃªu GB data tá»‘c Ä‘á»™ tiÃªu chuáº©n trong má»™t chu ká»³ 30 ngÃ y?", "HÃ£y so sÃ¡nh gÃ³i cÆ°á»›c V90B vÃ  V120B. Sá»± khÃ¡c biá»‡t vá» giÃ¡, data tá»‘c Ä‘á»™ cao (tÃ­nh theo tá»•ng chu ká»³) vÃ  phÃºt gá»i ngoáº¡i máº¡ng lÃ  gÃ¬?", "Liá»‡t kÃª táº¥t cáº£ cÃ¡c gÃ³i cÆ°á»›c cÃ³ chu ká»³ lá»›n hÆ¡n 30 ngÃ y (tá»©c lÃ  gÃ³i dÃ i háº¡n) VÃ€ cÃ³ Æ°u Ä‘Ã£i miá»…n phÃ­ gá»i ná»™i máº¡ng (cá»¥ thá»ƒ lÃ  Miá»…n phÃ­ cÃ¡c cuá»™c gá»i dÆ°á»›i 10 phÃºt hoáº·c tÆ°Æ¡ng Ä‘Æ°Æ¡ng) HOáº¶C Æ°u Ä‘Ã£i data tá»‘c Ä‘á»™ cao theo ngÃ y lÃ  1GB? Náº¿u khÃ´ng cÃ³ gÃ³i nÃ o, hÃ£y giáº£i thÃ­ch táº¡i sao.", "Má»™t ngÆ°á»i dÃ¹ng sá»­ dá»¥ng gÃ³i 12MXH100. Giáº£ sá»­ giÃ¡ cÆ°á»›c khÃ´ng Ä‘á»•i, náº¿u há» dÃ¹ng gÃ³i cÆ°á»›c ngáº¯n háº¡n MXH100 trong cÃ¹ng 360 ngÃ y Ä‘Ã³, há» sáº½ pháº£i tráº£ thÃªm/bá»›t bao nhiÃªu tiá»n?", "GÃ³i cÆ°á»›c nÃ o cÃ³ Æ°u Ä‘Ã£i Ä‘áº·c biá»‡t lÃ  miá»…n phÃ­ tháº£ ga truy cáº­p khÃ´ng giá»›i háº¡n vÃ  nhá»¯ng máº¡ng xÃ£ há»™i nÃ o Ä‘Æ°á»£c bao gá»“m trong Æ°u Ä‘Ã£i nÃ y? NÃªu cÃº phÃ¡p Ä‘Äƒng kÃ½ cá»§a gÃ³i cÆ°á»›c cÃ³ giÃ¡ 120.000 VNÄ cÃ³ Æ°u Ä‘Ã£i nÃ y.", "CÃ³ bao nhiÃªu gÃ³i cÆ°á»›c trong báº£ng khÃ´ng cÃ³ thÃ´ng tin chi tiáº¿t vá» cuá»™c gá»i ná»™i máº¡ng (nghÄ©a lÃ  cá»™t Gá»i ná»™i máº¡ng bá»‹ bá» trá»‘ng), vÃ  chÃºng lÃ  nhá»¯ng gÃ³i nÃ o?"]

    # for i, q in enumerate(q_array):
    #     print(f"| Index: {i},| Question: {q} , | Result: {qa(q)}")


    #12h46: Má»›i thay Ä‘á»•i Ä‘oáº¡n prompt vÃ  model cross_encoder