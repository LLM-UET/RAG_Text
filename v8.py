# import os
# from typing import List, Dict
# import csv, glob
# from langchain_core.documents import Document
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# from langchain_core.runnables import RunnablePassthrough
# from langchain_openai import ChatOpenAI
# from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain_openai import OpenAIEmbeddings
# from langchain_community.vectorstores import Chroma
# from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain_google_genai import ChatGoogleGenerativeAI


# from sentence_transformers import CrossEncoder

# def load_service_csv(path: str):
#     docs = []
#     with open(path, newline="", encoding="utf-8-sig") as f:
#         reader = csv.DictReader(f)
#         for row in reader:
#             def get(k): return row.get(k, "").strip()
#             code = get("MÃ£ dá»‹ch vá»¥")
#             text = (
#                 f"GÃ³i cÆ°á»›c {code} ({get('Thá»i gian thanh toÃ¡n')}). "
#                 f"GiÃ¡: {get('GiÃ¡ (VNÄ)')}Ä‘ / {get('Chu ká»³ (ngÃ y)')} ngÃ y. "
#                 f"4G tá»‘c Ä‘á»™ cao/ngÃ y: {get('4G tá»‘c Ä‘á»™ cao/ngÃ y')}. "
#                 f"Tá»± Ä‘á»™ng gia háº¡n: {get('Tá»± Ä‘á»™ng gia háº¡n')}. "
#                 f"CÃº phÃ¡p Ä‘Äƒng kÃ½: {get('CÃº phÃ¡p Ä‘Äƒng kÃ½')}. "
#             )
#             if get('Chi tiáº¿t'): text += f"Chi tiáº¿t: {get('Chi tiáº¿t')}. "
#             if get('Gá»i ná»™i máº¡ng'): text += f"Gá»i ná»™i máº¡ng: {get('Gá»i ná»™i máº¡ng')}. "
#             if get('Gá»i ngoáº¡i máº¡ng'): text += f"Gá»i ngoáº¡i máº¡ng: {get('Gá»i ngoáº¡i máº¡ng')}. "
#             docs.append(Document(page_content=text, metadata={"source": path, "service_code": code}))
#     return docs

# def chunk_documents(docs: List[Document], chunk_size=1024, chunk_overlap=100):
#     splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
#         chunk_size=chunk_size, chunk_overlap=chunk_overlap
#     )
#     return splitter.split_documents(docs)

# def build_vectorstore(docs: List[Document], persist_dir: str="chroma_db", use_openai=False):
#     if use_openai:
#         emb = OpenAIEmbeddings()
#     else:
#         emb = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
#     vect = Chroma.from_documents(documents=docs, embedding=emb, ids=[d.metadata["service_code"] + f"_{i}" for i, d in enumerate(docs)], persist_directory=persist_dir)
#     return vect

# def make_reranked_retriever(vectorstore, fetch_k=20, top_k=5):
#     """
#     1. Láº¥y trÆ°á»›c fetch_k candidates tá»« retriever gá»‘c
#     2. Rerank báº±ng cross-encoder (máº¡nh hÆ¡n cosine similarity)
#     3. Tráº£ vá» top_k tá»‘t nháº¥t
#     """
#     retriever = vectorstore.as_retriever(search_kwargs={"k": fetch_k})
#     reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

#     def retrieve(question: str) -> List[Document]:
#         candidates = retriever.invoke(question)
#         unique_candidates = []
#         seen = set()
#         for d in candidates:
#             code = d.metadata.get("service_code")
#             if code not in seen:
#                 unique_candidates.append(d)
#                 seen.add(code)
#         candidates = unique_candidates
#         if not candidates:
#             return []
#         pairs = [(question, d.page_content) for d in candidates]
#         scores = reranker.predict(pairs)
#         ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
#         reranked_docs = [doc for doc, _ in ranked[:top_k]]
#         return reranked_docs

#     return retrieve


# RAG_PROMPT = """
# ğŸ¯ Vai trÃ²:
# Báº¡n lÃ  má»™t **trá»£ lÃ½ áº£o thÃ´ng minh** cÃ³ nhiá»‡m vá»¥ há»— trá»£ khÃ¡ch hÃ ng vá» **cÃ¡c gÃ³i cÆ°á»›c cá»§a nhÃ  máº¡ng Viettel**.

# ---

# ğŸ§© Nhiá»‡m vá»¥:
# Báº¡n cáº§n **tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng chá»‰ dá»±a trÃªn dá»¯ liá»‡u Ä‘Ã£ cung cáº¥p trong "Ngá»¯ cáº£nh"**.  
# Ngá»¯ cáº£nh lÃ  cÆ¡ sá»Ÿ dá»¯ liá»‡u dáº¡ng báº£ng, má»—i hÃ ng tÆ°Æ¡ng á»©ng vá»›i má»™t gÃ³i cÆ°á»›c, cÃ¡c cá»™t mÃ´ táº£ thuá»™c tÃ­nh cá»¥ thá»ƒ.

# CÃ¡c cá»™t cá»§a báº£ng bao gá»“m:
# > MÃ£ dá»‹ch vá»¥, Thá»i gian thanh toÃ¡n, CÃ¡c dá»‹ch vá»¥ tiÃªn quyáº¿t, GiÃ¡ (VNÄ), Chu ká»³ (ngÃ y),
# > 4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/ngÃ y, 4G tá»‘c Ä‘á»™ cao/ngÃ y, 4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/chu ká»³, 4G tá»‘c Ä‘á»™ cao/chu ká»³,
# > Gá»i ná»™i máº¡ng, Gá»i ngoáº¡i máº¡ng, Tin nháº¯n, Chi tiáº¿t, Tá»± Ä‘á»™ng gia háº¡n, CÃº phÃ¡p Ä‘Äƒng kÃ½.

# Má»™t sá»‘ Ã´ cÃ³ thá»ƒ trá»‘ng (tÃ¹y gÃ³i cÆ°á»›c).

# ---

# ğŸ§  Ghi nhá»› quy táº¯c:
# 1. Náº¿u dá»¯ liá»‡u "4G tá»‘c Ä‘á»™ cao/ngÃ y" hoáº·c "4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/ngÃ y" lÃ  sá»‘ dÆ°Æ¡ng  
#    â†’ NghÄ©a lÃ  ngÆ°á»i dÃ¹ng Ä‘Æ°á»£c sá»­ dá»¥ng tá»‘i Ä‘a lÆ°á»£ng dá»¯ liá»‡u Ä‘Ã³ má»—i ngÃ y, sau Ä‘Ã³ reset vÃ o ngÃ y tiáº¿p theo.

# 2. Náº¿u **khÃ´ng cÃ³ dá»¯ liá»‡u theo ngÃ y**, nhÆ°ng cÃ³ dá»¯ liá»‡u theo chu ká»³  
#    â†’ NghÄ©a lÃ  toÃ n bá»™ dung lÆ°á»£ng Ä‘Ã³ dÃ¹ng chung cho toÃ n chu ká»³.

# 3. Khi ngÆ°á»i dÃ¹ng há»i vá» **dung lÆ°á»£ng data**, hÃ£y tra cá»©u cÃ¡c cá»™t sau:
#    - "4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/ngÃ y"
#    - "4G tá»‘c Ä‘á»™ cao/ngÃ y"
#    - "4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/chu ká»³"
#    - "4G tá»‘c Ä‘á»™ cao/chu ká»³"
#    - "Chi tiáº¿t"

# 4. Trong **má»i cÃ¢u tráº£ lá»i**, báº¡n pháº£i trÃ­ch dáº«n tá»‘i thiá»ƒu cÃ¡c cá»™t:
#    - "MÃ£ dá»‹ch vá»¥"
#    - "CÃº phÃ¡p Ä‘Äƒng kÃ½"
#    - "GiÃ¡ (VNÄ)"
#    - "Chi tiáº¿t"

# 5. Náº¿u ngÆ°á»i dÃ¹ng nÃ³i â€œÄ‘iá»‡n thoáº¡i cá»¥c gáº¡châ€, â€œnghe gá»i Ã­tâ€, hoáº·c â€œÃ­t dÃ¹ng máº¡ngâ€  
#    â†’ Hiá»ƒu lÃ  cáº§n **gá»£i Ã½ gÃ³i cÆ°á»›c ráº» nháº¥t** (tra cá»™t â€œGiÃ¡ (VNÄ)â€ Ä‘á»ƒ chá»n giÃ¡ nhá» nháº¥t).

# 6. Náº¿u ngÆ°á»i dÃ¹ng há»i **gÃ³i nÃ o ráº» hÆ¡n**,  
#    â†’ So sÃ¡nh cá»™t â€œGiÃ¡ (VNÄ)â€ giá»¯a cÃ¡c gÃ³i.

# 7. Náº¿u ngÆ°á»i dÃ¹ng há»i **gÃ³i nÃ o ráº» nháº¥t**,  
#    â†’ Chá»n gÃ³i cÃ³ giÃ¡ trá»‹ **MIN cá»§a cá»™t â€œGiÃ¡ (VNÄ)â€**.

# 8. Náº¿u cÃ³ **nhiá»u báº£n ghi trÃ¹ng láº·p**,  
#    â†’ Chá»‰ cáº§n tá»•ng há»£p vÃ  **tráº£ lá»i tÃ³m táº¯t ná»™i dung chÃ­nh má»™t láº§n**.

# ---

# ğŸš« Náº¿u khÃ´ng tÃ¬m tháº¥y cÃ¢u tráº£ lá»i rÃµ rÃ ng trong cÆ¡ sá»Ÿ dá»¯ liá»‡u, **hÃ£y tráº£ lá»i chÃ­nh xÃ¡c**:
# "TÃ´i khÃ´ng biáº¿t - vui lÃ²ng liÃªn há»‡ tá»•ng Ä‘Ã i 18001090 hoáº·c email support@telco.vn"

# ---

# ğŸ—£ï¸ YÃªu cáº§u Ä‘á»‹nh dáº¡ng cÃ¢u tráº£ lá»i:
# - Viáº¿t **ngáº¯n gá»n, dá»… hiá»ƒu cho ngÆ°á»i dÃ¹ng phá»• thÃ´ng**.
# - Giá»¯ nguyÃªn **ngÃ´n ngá»¯ cá»§a ngÆ°á»i dÃ¹ng** (Æ°u tiÃªn tiáº¿ng Viá»‡t).
# - Khi trÃ­ch dáº«n, **luÃ´n ghi rÃµ nguá»“n theo dáº¡ng [source: filename]**.
# - **Tuyá»‡t Ä‘á»‘i khÃ´ng suy luáº­n hoáº·c bá»‹a thÃ´ng tin** khÃ´ng cÃ³ trong dá»¯ liá»‡u.

# ---

# Ngá»¯ cáº£nh:
# {context}

# CÃ¢u há»i ngÆ°á»i dÃ¹ng:
# {question}

# Tráº£ lá»i:
# """


# prompt_template = ChatPromptTemplate.from_template(RAG_PROMPT)

# def make_qa_chain(vectorstore, model_name="gemini-2.0-flash", temperature=0.0, use_openai_llm=True):
#     if use_openai_llm:
#         llm = ChatGoogleGenerativeAI(model=model_name, temperature=temperature)
#     else:
#         raise NotImplementedError("Only OpenAI LLM supported here")

#     retriever_fn = make_reranked_retriever(vectorstore)

#     rag_chain_lcel = (
#         {
#             "context": retriever_fn,
#             "question": RunnablePassthrough()
#         }
#         | prompt_template
#         | llm
#         | StrOutputParser()
#     )

#     def answer_query(question: str, k:int=5):
#         docs = retriever_fn(question)

#         answer = rag_chain_lcel.invoke(question)

#         hallucinated = "[source:" not in answer
#         return {
#             "answer": answer,
#             "docs": docs,
#             "hallucinated": hallucinated
#         }

#     return answer_query

# if __name__ == "__main__":
#     docs = load_service_csv("viettel.csv")
#     chunks_docs = chunk_documents(docs)
#     vect = build_vectorstore(chunks_docs)
#     qa = make_qa_chain(vect)

#     q = "NÃªu cÃº phÃ¡p Ä‘Äƒng kÃ½ cá»§a gÃ³i cÆ°á»›c cÃ³ giÃ¡ 120.000 VNÄ vÃ  cÃ³ Æ°u Ä‘Ã£i miá»…n phÃ­ data"
#     result = qa(q)

#     print("QUESTION", q)
#     print("ANSWER:", result["answer"])
#     print("HALLUCINATED:", result["hallucinated"])
#     print("SOURCES:", [d.metadata for d in result["docs"]])

import os
import csv
import pandas as pd
from typing import List, Dict
from sentence_transformers import CrossEncoder
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI
from dotenv import load_dotenv



def load_csv(path: str):
    df = pd.read_csv(path)
    df.columns = [c.strip() for c in df.columns]
    return df

def df_to_documents(df: pd.DataFrame, source: str):
    docs = []
    for _, row in df.iterrows():
        code = row.get("MÃ£ dá»‹ch vá»¥", "")
        text = " ".join(f"{col}: {row[col]}" for col in df.columns)
        docs.append(Document(page_content=text, metadata={"source": source, "service_code": code}))
    # print("-"*10)
    # print("Docs:", docs)
    # print("-"*10)
    return docs



def chunk_documents(docs, chunk_size=512, chunk_overlap=50):
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap
    )
    return splitter.split_documents(docs)



def build_vectorstore(docs, persist_dir="chroma_db"):
    emb = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
    vect = Chroma.from_documents(
        docs,
        embedding=emb,
        ids=[d.metadata["service_code"] + f"_{i}" for i, d in enumerate(docs)],
        persist_directory=persist_dir,
    )
    return vect


def make_reranked_retriever(vectorstore, fetch_k=30, top_k=5):
    retriever = vectorstore.as_retriever(search_kwargs={"k": fetch_k})
    reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

    def retrieve(question):
        candidates = retriever.invoke(question)
        print("-"*10)
        print("Candidates:", candidates)
        print("-"*10)

        uniq, seen = [], set()
        for d in candidates:
            code = d.metadata["service_code"]
            if code not in seen:
                uniq.append(d)
                seen.add(code)
        candidates = uniq
        print("-"*10)
        print("Candidates:", candidates)
        print("-"*10)

        if not candidates:
            return []

        pairs = [(question, d.page_content) for d in candidates]
        scores = reranker.predict(pairs)
        ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
        print("-"*10)
        print("Rank: ",ranked)
        print("-"*10)
        return [doc for doc, _ in ranked[:top_k]]

    return retrieve



def parse_logical_conditions(question: str) -> dict:
    q = question.lower()
    cond = {}

    import re
    m = re.search(r"(\d{2,9})\s*(vnÄ‘|vnd|Ä‘)", q)
    if m:
        cond["GiÃ¡ (VNÄ)"] = int(m.group(1))

    if "miá»…n phÃ­ data" in q or "free data" in q:
        cond["free_data"] = True

    if "miá»…n phÃ­ truy cáº­p" in q or "khÃ´ng giá»›i háº¡n data" in q:
        cond["unlimited_data"] = True

    if "miá»…n phÃ­ gá»i ná»™i máº¡ng" in q:
        cond["free_onnet"] = True
    print("Condition:", cond)
    return cond


def apply_conditions(df: pd.DataFrame, cond: dict) -> pd.DataFrame:
    filtered = df.copy()

    if "GiÃ¡ (VNÄ)" in cond:
        filtered = filtered[filtered["GiÃ¡ (VNÄ)"] == cond["GiÃ¡ (VNÄ)"]]

    if cond.get("free_data"):
        filtered = filtered[
            filtered["Chi tiáº¿t"].str.contains("miá»…n phÃ­", case=False, na=False)
            | filtered["4G tá»‘c Ä‘á»™ cao/ngÃ y"].astype(str).str.contains("khÃ´ng giá»›i háº¡n", case=False, na=False)
        ]

    if cond.get("unlimited_data"):
        filtered = filtered[
            filtered["Chi tiáº¿t"].str.contains("khÃ´ng giá»›i háº¡n", case=False, na=False)
        ]

    if cond.get("free_onnet"):
        filtered = filtered[
            filtered["Gá»i ná»™i máº¡ng"].str.contains("miá»…n phÃ­", case=False, na=False)
        ]
    print("Filter:", filtered)
    return filtered




RAG_PROMPT = """
ğŸ¯ Vai trÃ²:
Báº¡n lÃ  má»™t **trá»£ lÃ½ áº£o thÃ´ng minh** cÃ³ nhiá»‡m vá»¥ há»— trá»£ khÃ¡ch hÃ ng vá» **cÃ¡c gÃ³i cÆ°á»›c cá»§a nhÃ  máº¡ng Viettel**.

---

ğŸ§© Nhiá»‡m vá»¥:
Báº¡n cáº§n **tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng chá»‰ dá»±a trÃªn dá»¯ liá»‡u Ä‘Ã£ cung cáº¥p trong "Ngá»¯ cáº£nh"**.  
Ngá»¯ cáº£nh lÃ  cÆ¡ sá»Ÿ dá»¯ liá»‡u dáº¡ng báº£ng, má»—i hÃ ng tÆ°Æ¡ng á»©ng vá»›i má»™t gÃ³i cÆ°á»›c, cÃ¡c cá»™t mÃ´ táº£ thuá»™c tÃ­nh cá»¥ thá»ƒ.

CÃ¡c cá»™t cá»§a báº£ng bao gá»“m:
> MÃ£ dá»‹ch vá»¥, Thá»i gian thanh toÃ¡n, CÃ¡c dá»‹ch vá»¥ tiÃªn quyáº¿t, GiÃ¡ (VNÄ), Chu ká»³ (ngÃ y),
> 4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/ngÃ y, 4G tá»‘c Ä‘á»™ cao/ngÃ y, 4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/chu ká»³, 4G tá»‘c Ä‘á»™ cao/chu ká»³,
> Gá»i ná»™i máº¡ng, Gá»i ngoáº¡i máº¡ng, Tin nháº¯n, Chi tiáº¿t, Tá»± Ä‘á»™ng gia háº¡n, CÃº phÃ¡p Ä‘Äƒng kÃ½.

Má»™t sá»‘ Ã´ cÃ³ thá»ƒ trá»‘ng (tÃ¹y gÃ³i cÆ°á»›c).

---

ğŸ§  Ghi nhá»› quy táº¯c:
1. Náº¿u dá»¯ liá»‡u "4G tá»‘c Ä‘á»™ cao/ngÃ y" hoáº·c "4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/ngÃ y" lÃ  sá»‘ dÆ°Æ¡ng  
   â†’ NghÄ©a lÃ  ngÆ°á»i dÃ¹ng Ä‘Æ°á»£c sá»­ dá»¥ng tá»‘i Ä‘a lÆ°á»£ng dá»¯ liá»‡u Ä‘Ã³ má»—i ngÃ y, sau Ä‘Ã³ reset vÃ o ngÃ y tiáº¿p theo.

2. Náº¿u **khÃ´ng cÃ³ dá»¯ liá»‡u theo ngÃ y**, nhÆ°ng cÃ³ dá»¯ liá»‡u theo chu ká»³  
   â†’ NghÄ©a lÃ  toÃ n bá»™ dung lÆ°á»£ng Ä‘Ã³ dÃ¹ng chung cho toÃ n chu ká»³.

3. Khi ngÆ°á»i dÃ¹ng há»i vá» **dung lÆ°á»£ng data**, hÃ£y tra cá»©u cÃ¡c cá»™t sau:
   - "4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/ngÃ y"
   - "4G tá»‘c Ä‘á»™ cao/ngÃ y"
   - "4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/chu ká»³"
   - "4G tá»‘c Ä‘á»™ cao/chu ká»³"
   - "Chi tiáº¿t"

4. Trong **má»i cÃ¢u tráº£ lá»i**, báº¡n pháº£i trÃ­ch dáº«n tá»‘i thiá»ƒu cÃ¡c cá»™t:
   - "MÃ£ dá»‹ch vá»¥"
   - "CÃº phÃ¡p Ä‘Äƒng kÃ½"
   - "GiÃ¡ (VNÄ)"
   - "Chi tiáº¿t"

5. Náº¿u ngÆ°á»i dÃ¹ng nÃ³i â€œÄ‘iá»‡n thoáº¡i cá»¥c gáº¡châ€, â€œnghe gá»i Ã­tâ€, hoáº·c â€œÃ­t dÃ¹ng máº¡ngâ€  
   â†’ Hiá»ƒu lÃ  cáº§n **gá»£i Ã½ gÃ³i cÆ°á»›c ráº» nháº¥t** (tra cá»™t â€œGiÃ¡ (VNÄ)â€ Ä‘á»ƒ chá»n giÃ¡ nhá» nháº¥t).

6. Náº¿u ngÆ°á»i dÃ¹ng há»i **gÃ³i nÃ o ráº» hÆ¡n**,  
   â†’ So sÃ¡nh cá»™t â€œGiÃ¡ (VNÄ)â€ giá»¯a cÃ¡c gÃ³i.

7. Náº¿u ngÆ°á»i dÃ¹ng há»i **gÃ³i nÃ o ráº» nháº¥t**,  
   â†’ Chá»n gÃ³i cÃ³ giÃ¡ trá»‹ **MIN cá»§a cá»™t â€œGiÃ¡ (VNÄ)â€**.

8. Náº¿u cÃ³ **nhiá»u báº£n ghi trÃ¹ng láº·p**,  
   â†’ Chá»‰ cáº§n tá»•ng há»£p vÃ  **tráº£ lá»i tÃ³m táº¯t ná»™i dung chÃ­nh má»™t láº§n**.

---

ğŸš« Náº¿u khÃ´ng tÃ¬m tháº¥y cÃ¢u tráº£ lá»i rÃµ rÃ ng trong cÆ¡ sá»Ÿ dá»¯ liá»‡u, **hÃ£y tráº£ lá»i chÃ­nh xÃ¡c**:
"TÃ´i khÃ´ng biáº¿t - vui lÃ²ng liÃªn há»‡ tá»•ng Ä‘Ã i 18001090 hoáº·c email support@telco.vn"

---

ğŸ—£ï¸ YÃªu cáº§u Ä‘á»‹nh dáº¡ng cÃ¢u tráº£ lá»i:
- Viáº¿t **ngáº¯n gá»n, dá»… hiá»ƒu cho ngÆ°á»i dÃ¹ng phá»• thÃ´ng**.
- Giá»¯ nguyÃªn **ngÃ´n ngá»¯ cá»§a ngÆ°á»i dÃ¹ng** (Æ°u tiÃªn tiáº¿ng Viá»‡t).
- Khi trÃ­ch dáº«n, **luÃ´n ghi rÃµ nguá»“n theo dáº¡ng [source: filename]**.
- **Tuyá»‡t Ä‘á»‘i khÃ´ng suy luáº­n hoáº·c bá»‹a thÃ´ng tin** khÃ´ng cÃ³ trong dá»¯ liá»‡u.

---

Ngá»¯ cáº£nh:
{context}

CÃ¢u há»i ngÆ°á»i dÃ¹ng:
{question}

Tráº£ lá»i:
"""

prompt_template = ChatPromptTemplate.from_template(RAG_PROMPT)



def make_qa_engine(df: pd.DataFrame, vectorstore):

    llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0)
    reranker = make_reranked_retriever(vectorstore)

    rag_chain = (
        {
            "context": reranker,
            "question": RunnablePassthrough()
        }
        | prompt_template
        | llm
        | StrOutputParser()
    )

    def answer(question: str):
        cond = parse_logical_conditions(question)

        df_filtered = apply_conditions(df, cond)

        if len(df_filtered) > 0:
            allowed_codes = set(df_filtered["MÃ£ dá»‹ch vá»¥"])
            def retriever_override(q):
                docs = reranker(q)
                return [d for d in docs if d.metadata["service_code"] in allowed_codes]

            final_chain = (
                {
                    "context": retriever_override,
                    "question": RunnablePassthrough()
                }
                | prompt_template
                | llm
                | StrOutputParser()
            )
            return final_chain.invoke(question)

        return rag_chain.invoke(question)

    return answer



if __name__ == "__main__":
    load_dotenv()
    api_key = os.getenv("GOOGLE_API_KEY")
    # os.environ["GOOGLE_API_KEY"] = "AIzaSyAGFAp3QFU55ktoGn2-5ZY4jb3xFK_HTrs"


    df = load_csv("viettel.csv")
    docs = df_to_documents(df, "viettel.csv")
    chunks = chunk_documents(docs)
    vect = build_vectorstore(chunks)

    qa = make_qa_engine(df, vect)

    # q = "NÃªu cÃº phÃ¡p Ä‘Äƒng kÃ½ cá»§a gÃ³i cÆ°á»›c cÃ³ giÃ¡ 120000 VNÄ vÃ  cÃ³ Æ°u Ä‘Ã£i miá»…n phÃ­ data"
    q = "Liá»‡t kÃª táº¥t cáº£ cÃ¡c gÃ³i cÆ°á»›c cÃ³ chu ká»³ lá»›n hÆ¡n 30 ngÃ y?"
    print("QUESTION:", q)
    print("ANSWER:", qa(q))

    # q_array = ["GÃ³i cÆ°á»›c SD70 cÃ³ giÃ¡ bao nhiÃªu vÃ  cung cáº¥p bao nhiÃªu GB data tá»‘c Ä‘á»™ tiÃªu chuáº©n trong má»™t chu ká»³ 30 ngÃ y?", "HÃ£y so sÃ¡nh gÃ³i cÆ°á»›c V90B vÃ  V120B. Sá»± khÃ¡c biá»‡t vá» giÃ¡, data tá»‘c Ä‘á»™ cao (tÃ­nh theo tá»•ng chu ká»³) vÃ  phÃºt gá»i ngoáº¡i máº¡ng lÃ  gÃ¬?", "Liá»‡t kÃª táº¥t cáº£ cÃ¡c gÃ³i cÆ°á»›c cÃ³ chu ká»³ lá»›n hÆ¡n 30 ngÃ y (tá»©c lÃ  gÃ³i dÃ i háº¡n) VÃ€ cÃ³ Æ°u Ä‘Ã£i miá»…n phÃ­ gá»i ná»™i máº¡ng (cá»¥ thá»ƒ lÃ  Miá»…n phÃ­ cÃ¡c cuá»™c gá»i dÆ°á»›i 10 phÃºt hoáº·c tÆ°Æ¡ng Ä‘Æ°Æ¡ng) HOáº¶C Æ°u Ä‘Ã£i data tá»‘c Ä‘á»™ cao theo ngÃ y lÃ  1GB? Náº¿u khÃ´ng cÃ³ gÃ³i nÃ o, hÃ£y giáº£i thÃ­ch táº¡i sao.", "Má»™t ngÆ°á»i dÃ¹ng sá»­ dá»¥ng gÃ³i 12MXH100. Giáº£ sá»­ giÃ¡ cÆ°á»›c khÃ´ng Ä‘á»•i, náº¿u há» dÃ¹ng gÃ³i cÆ°á»›c ngáº¯n háº¡n MXH100 trong cÃ¹ng 360 ngÃ y Ä‘Ã³, há» sáº½ pháº£i tráº£ thÃªm/bá»›t bao nhiÃªu tiá»n?", "GÃ³i cÆ°á»›c nÃ o cÃ³ Æ°u Ä‘Ã£i Ä‘áº·c biá»‡t lÃ  miá»…n phÃ­ tháº£ ga truy cáº­p khÃ´ng giá»›i háº¡n vÃ  nhá»¯ng máº¡ng xÃ£ há»™i nÃ o Ä‘Æ°á»£c bao gá»“m trong Æ°u Ä‘Ã£i nÃ y? NÃªu cÃº phÃ¡p Ä‘Äƒng kÃ½ cá»§a gÃ³i cÆ°á»›c cÃ³ giÃ¡ 120.000 VNÄ cÃ³ Æ°u Ä‘Ã£i nÃ y.", "CÃ³ bao nhiÃªu gÃ³i cÆ°á»›c trong báº£ng khÃ´ng cÃ³ thÃ´ng tin chi tiáº¿t vá» cuá»™c gá»i ná»™i máº¡ng (nghÄ©a lÃ  cá»™t Gá»i ná»™i máº¡ng bá»‹ bá» trá»‘ng), vÃ  chÃºng lÃ  nhá»¯ng gÃ³i nÃ o?"]
    # q_array = ["GÃ³i MXH120 cÃ³ pháº£i lÃ  gÃ³i tráº£ trÆ°á»›c khÃ´ng?", "GÃ³i MXH100 cÃ³ tá»± Ä‘á»™ng gia háº¡n khÃ´ng?", "GÃ³i V120B cÃ³ Ä‘Æ°á»£c miá»…n phÃ­ gá»i ná»™i máº¡ng khÃ´ng?", "GÃ³i MXH120 cÃ³ ná»™i dung gÃ¬?", "GiÃ¡ cá»§a gÃ³i cÆ°á»›c MXH100 lÃ  bao nhiÃªu?", "Tin nháº¯n cá»§a gÃ³i VB90 lÃ  gÃ¬?", "Äá»ƒ Ä‘Äƒng kÃ½ gÃ³i MXH120 thÃ¬ soáº¡n tin gá»­i 191 Ä‘Ãºng khÃ´ng?", "Äá»ƒ Ä‘Äƒng kÃ½ gÃ³i MXH100 thÃ¬ soáº¡n tin gá»­i 290 Ä‘Ãºng khÃ´ng?"]

    # for i, q in enumerate(q_array):
    #     print(f"| Index: {i},| Question: {q} , | Result: {qa(q)}")
