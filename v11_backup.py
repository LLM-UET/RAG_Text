import os
import csv
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple
from sentence_transformers import CrossEncoder
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI
from rag_reasoning import (ReasoningDataQueryEngine,ReasoningDataQuery)
# from reasoning import (ReasoningDataQueryEngine,ReasoningDataQuery)
from dotenv import load_dotenv

DEBUG = True
FETCH_K = 20
TOP_K = 5 
CONFIDENCE_THRESHOLD = 1.8

def load_csv(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    df.columns = [c.strip() for c in df.columns]
    df = df.fillna("")
    df["Gi√° (VNƒê)"] = df["Gi√° (VNƒê)"].apply(lambda x: int(x) if str(x).strip() != "" else 0)
    df["Chu k·ª≥ (ng√†y)"] = df["Chu k·ª≥ (ng√†y)"].apply(lambda x: int(x) if str(x).strip() != "" else 0)
    return df

def df_to_documents(df: pd.DataFrame, source: str) -> List[Document]:
    docs = []
    for _, row in df.iterrows():
        code = str(row.get("M√£ d·ªãch v·ª•", "")).strip()
        text_parts = []
        for col in df.columns:
            val = row[col]
            text_parts.append(f"{col}: {val}")
        text = " . ".join(text_parts)
        docs.append(Document(page_content=text, metadata={"source": source, "service_code": code}))
    return docs



def chunk_documents(docs: List[Document], chunk_size=512, chunk_overlap=50):
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap
    )
    return splitter.split_documents(docs)



def build_vectorstore(docs: List[Document], persist_dir="chroma_db"):
    emb = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
    vect = Chroma.from_documents(
        docs,
        embedding=emb,
        ids = [f"{d.metadata.get('service_code','unknown')}_{i}" for i, d in enumerate(docs)],
        persist_directory=persist_dir,
    )
    return vect

def write_local_knowledge_reasoning_query(question: str) -> str | None:
        prompt_temp = """
        B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o th√¥ng minh c·ªßa nh√† m·∫°ng Viettel, c√≥ kh·∫£ nƒÉng gi·∫£i ƒë√°p th·∫Øc m·∫Øc c·ªßa ng∆∞·ªùi d√πng.
        Nhi·ªám v·ª•: X√°c ƒë·ªãnh xem b·∫°n c√≥ th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m√† ch·ªâ d·ª±a theo ki·∫øn th·ª©c ƒë√£ cho hay kh√¥ng, b·∫±ng c√°ch truy v·∫•n t·ª´ c∆° s·ªü d·ªØ li·ªáu.
        B·∫°n ƒë∆∞·ª£c cung c·∫•p m·ªôt c∆° s·ªü d·ªØ li·ªáu c√°c g√≥i c∆∞·ªõc (g·ªçi t·∫Øt l√† g√≥i) c·ªßa Viettel. ƒê√¢y l√† c∆° s·ªü d·ªØ li·ªáu d·∫°ng b·∫£ng, m·ªói h√†ng ch·ª©a th√¥ng tin c·ªßa m·ªôt g√≥i, m·ªói c·ªôt ch·ª©a thu·ªôc t√≠nh c·ª• th·ªÉ c·ªßa g√≥i ƒë√≥.
        M·ªôt s·ªë h√†ng c√≥ th·ªÉ tr·ªëng (optional).
        C√°c c·ªôt c·ªßa b·∫£ng bao g·ªìm:
        > M√£ d·ªãch v·ª•, Th·ªùi gian thanh to√°n, C√°c d·ªãch v·ª• ti√™n quy·∫øt, Gi√° (VNƒê), Chu k·ª≥ (ng√†y),
        > 4G t·ªëc ƒë·ªô ti√™u chu·∫©n/ng√†y, 4G t·ªëc ƒë·ªô cao/ng√†y, 4G t·ªëc ƒë·ªô ti√™u chu·∫©n/chu k·ª≥, 4G t·ªëc ƒë·ªô cao/chu k·ª≥,
        > G·ªçi n·ªôi m·∫°ng, G·ªçi ngo·∫°i m·∫°ng, Tin nh·∫Øn, Chi ti·∫øt, T·ª± ƒë·ªông gia h·∫°n, C√∫ ph√°p ƒëƒÉng k√Ω.

        Ch√∫ √Ω 1: N·∫øu d·ªØ li·ªáu theo ng√†y l√† s·ªë d∆∞∆°ng th√¨ nghƒ©a l√† m·ªôt ng√†y ng∆∞·ªùi d√πng ch·ªâ ƒë∆∞·ª£c d√πng t·ªëi ƒëa b·∫•y nhi√™u d·ªØ li·ªáu m√† th√¥i, sang ng√†y kh√°c l·∫°i ƒë∆∞·ª£c th√™m. C√≤n n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu theo ng√†y th√¨ nghƒ©a l√† ng∆∞·ªùi d√πng ƒë∆∞·ª£c d√πng tho·∫£i m√°i to√†n b·ªô d·ªØ li·ªáu trong chu k·ª≥ m√† kh√¥ng b·ªã gi·ªõi h·∫°n theo ng√†y, cho ƒë·∫øn khi h·∫øt d·ªØ li·ªáu trong chu k·ª≥ ƒë√≥ th√¨ ph·∫£i ch·ªù chu k·ª≥ ti·∫øp theo (n·∫øu gia h·∫°n) m·ªõi ƒë∆∞·ª£c ti·∫øp t·ª•c s·ª≠ d·ª•ng.
        Ch√∫ √Ω 1b: N·∫øu ng∆∞·ªùi d√πng h·ªèi dung l∆∞·ª£ng th√¨ c·∫ßn ch·ªçn c√°c c·ªôt sau: "4G t·ªëc ƒë·ªô ti√™u chu·∫©n/ng√†y", "4G t·ªëc ƒë·ªô cao/ng√†y", "4G t·ªëc ƒë·ªô ti√™u chu·∫©n/chu k·ª≥", "4G t·ªëc ƒë·ªô cao/chu k·ª≥", "Chi ti·∫øt".
        Ch√∫ √Ω 2: B·∫°n ph·∫£i lu√¥n SELECT c√°c c·ªôt sau trong m·ªçi tr∆∞·ªùng h·ª£p: "M√£ d·ªãch v·ª•", "C√∫ ph√°p", "Gi√° (VNƒê)", "Chi ti·∫øt".
        Ch√∫ √Ω 3: N·∫øu ng∆∞·ªùi d√πng nh·ªù t∆∞ v·∫•n cho ƒëi·ªán tho·∫°i c·ª•c g·∫°ch, nghe g·ªçi √≠t ho·∫∑c √≠t s·ª≠ d·ª•ng m·∫°ng... th√¨ b·∫°n c·∫ßn hi·ªÉu l√† ph·∫£i t√¨m g√≥i c∆∞·ªõc r·∫ª nh·∫•t.

        C√∫ ph√°p truy c·∫≠p l·∫•y d·ªØ li·ªáu t·ª´ c∆° s·ªü d·ªØ li·ªáu nh∆∞ sau:
        SELECT "T√™n c·ªôt 1", "T√™n c·ªôt 2"
        WHERE "T√™n c·ªôt 3" = "Gi√° tr·ªã 3" AND "T√™n c·ªôt 4" > "Gi√° tr·ªã 4"...
        OR "T√™n c·ªôt 5" < "Gi√° tr·ªã 5" AND "T√™n c·ªôt 6" <= "Gi√° tr·ªã 6"...
        OR "T√™n c·ªôt 7" REACHES MIN
        OR "T√™n c·ªôt 8" REACHES MAX
        OR "T√™n c·ªôt 9" CONTAINS "Gi√° tr·ªã 9"...
        ...

        trong ƒë√≥ t√™n c·ªôt v√† gi√° tr·ªã lu√¥n ·ªü trong d·∫•u nh√°y (") cho d√π ƒë√≥ l√† gi√° tr·ªã s·ªë ƒëi chƒÉng n·ªØa (ch·∫≥ng h·∫°n "6").
        T√™n c·ªôt c≈©ng nh∆∞ gi√° tr·ªã s·∫Ω kh√¥ng bao gi·ªù v√† kh√¥ng ƒë∆∞·ª£c ch·ª©a m·ªôt d·∫•u nh√°y kh√°c trong ƒë√≥, n·∫øu kh√¥ng truy v·∫•n s·∫Ω b·ªã coi l√† sai. V√≠ d·ª• "6"" l√† sai.
        B·∫°n kh√¥ng c·∫ßn vi·∫øt nh·ªØng ƒëi·ªÅu ki·ªán lo·∫°i b·ªè d·ªØ li·ªáu sai v√≠ d·ª• "4G t·ªëc ƒë·ªô cao/chu k·ª≥" > 0. H√£y m·∫∑c ƒë·ªãnh d·ªØ li·ªáu lu√¥n ƒë√∫ng.
        B·∫°n kh√¥ng ƒë∆∞·ª£c ph√©p d√πng d·∫•u ngo·∫∑c ƒë∆°n nh∆∞ n√†y ( ho·∫∑c nh∆∞ n√†y ) ƒë·ªÉ nh√≥m c√°c bi·ªÉu th·ª©c logic AND-OR. H√£y c·ªë g·∫Øng "ph√° ngo·∫∑c" ƒë·ªÉ vi·∫øt l·∫°i c√¢u truy v·∫•n cho d·ªÖ hi·ªÉu h∆°n nh√©.
        Th·ª© t·ª± ∆∞u ti√™n lu√¥n l√† AND tr∆∞·ªõc r·ªìi m·ªõi ƒë·∫øn OR.
        B·∫°n c≈©ng kh√¥ng ƒë∆∞·ª£c ph√©p d√πng c√°c to√°n t·ª≠ so s√°nh kh√°c ngo√†i =, >, <, >=, <=, REACHES MIN, REACHES MAX, CONTAINS.
        B·∫°n c≈©ng kh√¥ng ƒë∆∞·ª£c ph√©p d√πng c√°c to√°n t·ª≠ logic kh√°c ngo√†i AND, OR.
        B·∫°n c≈©ng kh√¥ng ƒë∆∞·ª£c ph√©p d√πng c√°c to√°n t·ª≠ kh√°c ngo√†i SELECT, WHERE.
        M·ªánh ƒë·ªÅ WHERE l√† b·∫Øt bu·ªôc.
        Khi ng∆∞·ªùi d√πng h·ªèi gi√° r·∫ª, gi√° r·∫ª nh·∫•t th√¨ c·∫ßn vi·∫øt query theo ki·ªÉu "Gi√° (VNƒê)" REACHES MIN, ch·ª© kh√¥ng ƒë∆∞·ª£c so s√°nh v·ªõi m·ªôt gi√° tr·ªã c·ª• th·ªÉ n√†o ƒë√≥, ch·∫≥ng h·∫°n "Gi√° (VNƒê)" < 100000.
        Tuy nhi√™n n·∫øu ng∆∞·ªùi d√πng h·ªèi "gi√° r·∫ª h∆°n" th√¨ ph·∫£i d·ª±a v√†o l·ªãch s·ª≠ chat ƒë·ªÉ bi·∫øt ng∆∞·ªùi d√πng ƒëang n√≥i t·ªõi nh·ªØng g√≥i n√†o, sau ƒë√≥ x√°c ƒë·ªãnh g√≥i r·∫ª h∆°n trong c√°c g√≥i ƒë√≥.
        N·∫øu ng∆∞·ªùi d√πng h·ªèi "nhi·ªÅu data", "data kh√¥ng gi·ªõi h·∫°n", "mi·ªÖn ph√≠"... th√¨ n√™n ch·ªçn c√°c g√≥i c√≥ "4G t·ªëc ƒë·ªô ti√™u chu·∫©n/ng√†y" REACHES MIN ho·∫∑c "4G t·ªëc ƒë·ªô cao/ng√†y" REACHES MAX, ho·∫∑c c·ªôt "Chi ti·∫øt" CONTAINS "kh√¥ng gi·ªõi h·∫°n", "th·∫£ ga", "mi·ªÖn ph√≠" .v.v.

        Trong tr∆∞·ªùng h·ª£p b·∫°n c√≥ th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng b·∫±ng c√°ch t·∫°o m·ªôt truy v·∫•n d·ªØ li·ªáu nh∆∞ tr√™n, h√£y tr·∫£ v·ªÅ truy v·∫•n.
        N·∫øu kh√¥ng t·∫°o ƒë∆∞·ª£c truy v·∫•n nh∆∞ng c√¢u h·ªèi v·∫´n thu·ªôc ph·∫°m vi th√¥ng tin g√≥i c∆∞·ªõc, sim th·∫ª, nh√† m·∫°ng, gi√° c·∫£... th√¨ tr·∫£ v·ªÅ:
            SELECT "M√£ d·ªãch v·ª•", "C√∫ ph√°p", "Gi√° (VNƒê)", "Chi ti·∫øt" WHERE "Chi ti·∫øt" CONTAINS "<c√°c t·ª´ kh√≥a trong c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng>"
        N·∫øu c√¢u h·ªèi ho√†n to√†n n·∫±m ngo√†i ph·∫°m vi nh·ªØng th√¥ng tin g√≥i c∆∞·ªõc, sim th·∫ª... nh∆∞ tr√™n th√¨ tr·∫£ v·ªÅ IMPOSSIBLE.

        H√£y nghi√™n c·ª©u c√°c v√≠ d·ª• d∆∞·ªõi ƒë√¢y, v√† tr·∫£ l·ªùi c√¢u h·ªèi ƒë∆∞·ª£c ƒë∆∞a ra ·ªü cu·ªëi c√πng:
        V√≠ d·ª• 1:
        - C√¢u h·ªèi: G√≥i c∆∞·ªõc n√†o c√≥ gi√° r·∫ª nh·∫•t?
        - Tr·∫£ l·ªùi: SELECT "M√£ d·ªãch v·ª•", "Gi√° (VNƒê)" WHERE "Gi√° (VNƒê)" REACHES MIN
        V√≠ d·ª• 2:
        - C√¢u h·ªèi: L√†m th·∫ø n√†o ƒë·ªÉ ƒëƒÉng k√Ω d·ªãch v·ª• SD70?
        - Tr·∫£ l·ªùi: SELECT "Chi ti·∫øt", "C√∫ ph√°p", "M√£ d·ªãch v·ª•" WHERE "M√£ d·ªãch v·ª•" = "SD70"
        V√≠ d·ª• 3:
        - C√¢u h·ªèi: B·∫°n ∆°i th·∫ø sao thu√™ bao c·ªßa t√¥i c·ª© t·ª± tr·ª´ ti·ªÅn th·∫ø nh·ªâ, b·∫°n xem gi√∫p t√¥i s·ªë d∆∞ c√≤n bao nhi√™u v·ªõi
        - Tr·∫£ l·ªùi: IMPOSSIBLE
        V√≠ d·ª• 4:
        - C√¢u h·ªèi: ·ª™ th·∫ø xem gi√∫p t√¥i g√≥i n√†o ƒë·ªÉ anh l∆∞·ªõt m·∫°ng th·∫£ ga ƒëi, m·ªôt ng√†y xem phim ƒë√£ t·ªën m·∫•y gigabyte r·ªìi
        - Tr·∫£ l·ªùi: SELECT "M√£ d·ªãch v·ª•", "4G t·ªëc ƒë·ªô ti√™u chu·∫©n/ng√†y" WHERE "Chi ti·∫øt" CONTAINS "l∆∞·ªõt m·∫°ng th·∫£ ga" AND "4G t·ªëc ƒë·ªô ti√™u chu·∫©n/ng√†y" REACHES MIN
        V√≠ d·ª• 5:
        - C√¢u h·ªèi: √Ä b·∫°n ∆°i b√™n b·∫°n c√≥ g√≥i n√†o r·∫ª m√† l∆∞·ªõt m·∫°ng tho·∫£i m√°i kh√¥ng, ch·ª© m·ªôt ng√†y t√¥i l∆∞·ªõt m·∫°ng h·∫øt m·∫•y gigabyte r·ªìi
        - Tr·∫£ l·ªùi: SELECT "M√£ d·ªãch v·ª•", "4G t·ªëc ƒë·ªô ti√™u chu·∫©n/ng√†y" WHERE "Chi ti·∫øt" CONTAINS "l∆∞·ªõt m·∫°ng tho·∫£i m√°i" AND "4G t·ªëc ƒë·ªô ti√™u chu·∫©n/ng√†y" REACHES MIN AND "Gi√° (VNƒê)" REACHES MIN

        C√¢u h·ªèi: {question}
        """
        # print(f"write_local_knowledge_reasoning_query: prompt: {prompt_temp}")
        print(f"write_local_knowledge_reasoning_query: calling...")
        # prompt_template_query = ChatPromptTemplate.from_template(prompt_temp)
        llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.0)

        prompt = prompt_temp.format(question=question)
        response = llm.invoke(prompt)
        print(f"write_local_knowledge_reasoning_query: response: {response}")
        response_text = response.content.strip()
        # return None if "impossible" in response.content.strip().lower() else response.strip()
        return None if "impossible" in response_text.lower() else response_text

def make_reranked_retriever(df,vectorstore, fetch_k=30, top_k=5):
    retriever = vectorstore.as_retriever(search_kwargs={"k": fetch_k})
    reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
    emb = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
    reasoning_engine = ReasoningDataQueryEngine(df=df, embedder=emb)

    
    def retrieve_with_scores(question: str) -> List[Tuple[Document, float]]:
        # get candidates (list of Document)
        candidates = retriever.invoke(question)
        if DEBUG:
            print(f"[DEBUG] raw candidates count: {len(candidates)}")

        uniq = []
        seen = set()
        for d in candidates:
            code = d.metadata.get("service_code")
            if code not in seen:
                uniq.append(d)
                seen.add(code)
        candidates = uniq
        if DEBUG:
            print(f"[DEBUG] deduped candidates count: {len(candidates)}")
            for d in candidates[:10]:
                meta = d.metadata
                print("[CAND]", meta, "->", d.page_content[:120].replace("\n"," "))
        if not candidates:
            return []

        pairs = [(question, d.page_content) for d in candidates]
        scores = reranker.predict(pairs)  # array of floats
        scored = list(zip(candidates, [float(s) for s in scores]))
        scored_sorted = sorted(scored, key=lambda x: x[1], reverse=True)
        if DEBUG:
            print("[DEBUG] top scored (doc, score):")
            for doc, sc in scored_sorted[:min(10, len(scored_sorted))]:
                print(f"  {doc.metadata.get('service_code')} -> {sc:.4f}")

        q=write_local_knowledge_reasoning_query(question)
        if DEBUG:
            print("SQL: ", q)
        if q is None or q.strip() == "IMPOSSIBLE":
            return {"type": "docs", "docs": scored_sorted[:top_k]}
    
        try:
            queryObject = reasoning_engine.compile(q)
            table = reasoning_engine.apply(queryObject)
            if DEBUG:
                print("Table:", table)
            return {
                "type": "table",
                "table": table,
                "queryObject": queryObject
            }
        except Exception as e:
            return {"type":"docs", "docs": scored_sorted[:top_k]}
        
        
        # try:
        #     queryObject = reasoning_engine.compile(q)
        #     print("Query Object: ", queryObject)
        # except Exception as e:
        #     print(f"check_local_knowledge_reasoning: query compilation failed: {e}")
        #     print(f"Initial query written by LLM: {q}")
        #     return None

        # try:
        #     queryObject = reasoning_engine.compile(q)
        #     print("Query Object: ", queryObject)
        # except Exception as e:
        #     print(f"check_local_knowledge_reasoning: query compilation failed: {e}")
        #     print(f"Initial query written by LLM: {q}")
        #     return None
        
        # table = reasoning_engine.apply(queryObject)
        # if DEBUG:
        #     print(f"check_local_knowledge_reasoning: applied table: {table}")
        
        # return scored_sorted[:top_k]

    return retrieve_with_scores





RAG_PROMPT = """
üéØ Vai tr√≤:
B·∫°n l√† m·ªôt **tr·ª£ l√Ω ·∫£o th√¥ng minh** c√≥ nhi·ªám v·ª• h·ªó tr·ª£ kh√°ch h√†ng v·ªÅ **c√°c g√≥i c∆∞·ªõc c·ªßa nh√† m·∫°ng Viettel**.

---

üß© Nhi·ªám v·ª•:
B·∫°n c·∫ßn **tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ch·ªâ d·ª±a tr√™n d·ªØ li·ªáu ƒë√£ cung c·∫•p trong "Ng·ªØ c·∫£nh"**.  
Ng·ªØ c·∫£nh l√† c∆° s·ªü d·ªØ li·ªáu d·∫°ng b·∫£ng, m·ªói h√†ng t∆∞∆°ng ·ª©ng v·ªõi m·ªôt g√≥i c∆∞·ªõc, c√°c c·ªôt m√¥ t·∫£ thu·ªôc t√≠nh c·ª• th·ªÉ.

C√°c c·ªôt c·ªßa b·∫£ng bao g·ªìm:
> M√£ d·ªãch v·ª•, Th·ªùi gian thanh to√°n, C√°c d·ªãch v·ª• ti√™n quy·∫øt, Gi√° (VNƒê), Chu k·ª≥ (ng√†y),
> 4G t·ªëc ƒë·ªô ti√™u chu·∫©n/ng√†y, 4G t·ªëc ƒë·ªô cao/ng√†y, 4G t·ªëc ƒë·ªô ti√™u chu·∫©n/chu k·ª≥, 4G t·ªëc ƒë·ªô cao/chu k·ª≥,
> G·ªçi n·ªôi m·∫°ng, G·ªçi ngo·∫°i m·∫°ng, Tin nh·∫Øn, Chi ti·∫øt, T·ª± ƒë·ªông gia h·∫°n, C√∫ ph√°p ƒëƒÉng k√Ω.

M·ªôt s·ªë √¥ c√≥ th·ªÉ tr·ªëng (t√πy g√≥i c∆∞·ªõc).

---

üß† Ghi nh·ªõ quy t·∫Øc:
1. N·∫øu d·ªØ li·ªáu "4G t·ªëc ƒë·ªô cao/ng√†y" ho·∫∑c "4G t·ªëc ƒë·ªô ti√™u chu·∫©n/ng√†y" l√† s·ªë d∆∞∆°ng  
   ‚Üí Nghƒ©a l√† ng∆∞·ªùi d√πng ƒë∆∞·ª£c s·ª≠ d·ª•ng t·ªëi ƒëa l∆∞·ª£ng d·ªØ li·ªáu ƒë√≥ m·ªói ng√†y, sau ƒë√≥ reset v√†o ng√†y ti·∫øp theo.

2. N·∫øu **kh√¥ng c√≥ d·ªØ li·ªáu theo ng√†y**, nh∆∞ng c√≥ d·ªØ li·ªáu theo chu k·ª≥  
   ‚Üí Nghƒ©a l√† to√†n b·ªô dung l∆∞·ª£ng ƒë√≥ d√πng chung cho to√†n chu k·ª≥.

3. Khi ng∆∞·ªùi d√πng h·ªèi v·ªÅ **dung l∆∞·ª£ng data**, h√£y tra c·ª©u c√°c c·ªôt sau:
   - "4G t·ªëc ƒë·ªô ti√™u chu·∫©n/ng√†y"
   - "4G t·ªëc ƒë·ªô cao/ng√†y"
   - "4G t·ªëc ƒë·ªô ti√™u chu·∫©n/chu k·ª≥"
   - "4G t·ªëc ƒë·ªô cao/chu k·ª≥"
   - "Chi ti·∫øt"

4. Trong **m·ªçi c√¢u tr·∫£ l·ªùi**, b·∫°n ph·∫£i tr√≠ch d·∫´n t·ªëi thi·ªÉu c√°c c·ªôt:
   - "M√£ d·ªãch v·ª•"
   - "C√∫ ph√°p ƒëƒÉng k√Ω"
   - "Gi√° (VNƒê)"
   - "Chi ti·∫øt"

5. N·∫øu ng∆∞·ªùi d√πng n√≥i ‚Äúƒëi·ªán tho·∫°i c·ª•c g·∫°ch‚Äù, ‚Äúnghe g·ªçi √≠t‚Äù, ho·∫∑c ‚Äú√≠t d√πng m·∫°ng‚Äù  
   ‚Üí Hi·ªÉu l√† c·∫ßn **g·ª£i √Ω g√≥i c∆∞·ªõc r·∫ª nh·∫•t** (tra c·ªôt ‚ÄúGi√° (VNƒê)‚Äù ƒë·ªÉ ch·ªçn gi√° nh·ªè nh·∫•t).

6. N·∫øu ng∆∞·ªùi d√πng h·ªèi **g√≥i n√†o r·∫ª h∆°n**,  
   ‚Üí So s√°nh c·ªôt ‚ÄúGi√° (VNƒê)‚Äù gi·ªØa c√°c g√≥i.

7. N·∫øu ng∆∞·ªùi d√πng h·ªèi **g√≥i n√†o r·∫ª nh·∫•t**,  
   ‚Üí Ch·ªçn g√≥i c√≥ gi√° tr·ªã **MIN c·ªßa c·ªôt ‚ÄúGi√° (VNƒê)‚Äù**.

8. N·∫øu c√≥ **nhi·ªÅu b·∫£n ghi tr√πng l·∫∑p**,  
   ‚Üí Ch·ªâ c·∫ßn t·ªïng h·ª£p v√† **tr·∫£ l·ªùi t√≥m t·∫Øt n·ªôi dung ch√≠nh m·ªôt l·∫ßn**.

---

üö´ N·∫øu kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi r√µ r√†ng trong c∆° s·ªü d·ªØ li·ªáu, **h√£y tr·∫£ l·ªùi ch√≠nh x√°c**:
"T√¥i kh√¥ng bi·∫øt - vui l√≤ng li√™n h·ªá t·ªïng ƒë√†i 18001090 ho·∫∑c email support@telco.vn"

---

üó£Ô∏è Y√™u c·∫ßu ƒë·ªãnh d·∫°ng c√¢u tr·∫£ l·ªùi:
- Vi·∫øt **ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu cho ng∆∞·ªùi d√πng ph·ªï th√¥ng**.
- Gi·ªØ nguy√™n **ng√¥n ng·ªØ c·ªßa ng∆∞·ªùi d√πng** (∆∞u ti√™n ti·∫øng Vi·ªát).
- Khi tr√≠ch d·∫´n, **lu√¥n ghi r√µ ngu·ªìn theo d·∫°ng [source: filename]**.
- **Tuy·ªát ƒë·ªëi kh√¥ng suy lu·∫≠n ho·∫∑c b·ªãa th√¥ng tin** kh√¥ng c√≥ trong d·ªØ li·ªáu.

---

Ng·ªØ c·∫£nh:
{context}

C√¢u h·ªèi ng∆∞·ªùi d√πng:
{question}

Tr·∫£ l·ªùi:
"""

prompt_template = ChatPromptTemplate.from_template(RAG_PROMPT)



def make_qa_chain(vectorstore, df, model_name:str="gemini-2.0-flash", temperature:float=0.0, fetch_k:int=FETCH_K, top_k:int=TOP_K, confidence_threshold:float=CONFIDENCE_THRESHOLD):
    llm = ChatGoogleGenerativeAI(model=model_name, temperature=temperature)
    # retriever_with_scores = make_reranked_retriever(df, vectorstore, fetch_k=fetch_k, top_k=top_k)
    retriever_handler = make_reranked_retriever(df, vectorstore, fetch_k=fetch_k, top_k=top_k)
    

    def answer_query(question: str):
        result = retriever_handler(question)

        if result["type"] == "table":
            table = result["table"]
            query_obj = result["queryObject"]

            try:
                table_md = table.to_markdown(index=False)
            except:
                table_md = str(table)

            prompt_case1 = f"""
            B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o th√¥ng minh, l√† nh√¢n vi√™n chƒÉm s√≥c kh√°ch h√†ng c·ªßa Viettel. B·∫°n c√≥ kh·∫£ nƒÉng tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n b·∫£ng d·ªØ li·ªáu ƒë√£ cho.
            B·∫¢NG D·ªÆ LI·ªÜU:
            {table_md}
            C√ÇU H·ªéI:
            {question}
            """
            if DEBUG:
                print("prompt_case1", prompt_case1)
            out = llm.invoke(prompt_case1)
            if DEBUG:
                print("Output", out)
            answer = out.content.strip()

            return { "answer": answer, "docs": [], "hallucinated": False }
        else: 
            # scored = retriever_with_scores(question)  # List[(Document, score)]
            scored = retriever_handler(question)
            if not scored:
                if DEBUG: print("[DEBUG] No candidates -> fallback")
                return {"answer": "T√¥i kh√¥ng bi·∫øt - vui l√≤ng li√™n h·ªá t·ªïng ƒë√†i 18001090 ho·∫∑c email support@telco.vn", "docs": [], "hallucinated": False}
            
            print("DEBUG scored =", scored)
            print("First element =", scored[0], "len =", len(scored[0]))
            docs, scores = zip(*scored)
            # Confidence check: use top score (or average) to decide
            top_score = float(scores[0])
            avg_score = float(sum(scores)/len(scores))
            if DEBUG:
                print(f"[DEBUG] top_score={top_score:.4f}, avg_score={avg_score:.4f}, threshold={confidence_threshold}")

            if top_score < confidence_threshold:
                if DEBUG: print("[DEBUG] top score below threshold -> fallback")
                return {"answer": "T√¥i kh√¥ng bi·∫øt - vui l√≤ng li√™n h·ªá t·ªïng ƒë√†i 18001090 ho·∫∑c email support@telco.vn", "docs": [d.metadata for d in docs], "hallucinated": False}

            # Build the context: include service_code and page_content for each doc
            context_sections = []
            for d in docs:
                src = d.metadata.get("source", "unknown")
                code = d.metadata.get("service_code", "")
                # table_text = d.metadata.get("filter_table", "")
                # context_sections.append(f"[source: {src} | service_code: {code}]\n{d.page_content}\n{table_text}")
                context_sections.append(f"[source: {src} | service_code: {code}]\n{d.page_content}")
            context = "\n\n".join(context_sections)

            # filtered_table_text = ""
            # for d in docs:
            #     if "filtered_table" in d.metadata:
            #         filtered_table_text = d.metadata["filtered_table"]
            #         break

            # # --- 5. Build full context ---
            # full_context = context
            # if filtered_table_text:
            #     full_context += "\n\n" + filtered_table_text

            # Build prompt and invoke LLM
            prompt = RAG_PROMPT.format(context=context, question=question)
            # if DEBUG:
            #     print("=== Prompt to LLM ===")
            #     print(prompt[:2000])  # only print head
            #     print("=== End Prompt ===")
            out = llm.invoke(prompt)
            # ChatGoogleGenerativeAI returns an object; try to extract string
            answer = out.content.strip() if hasattr(out, "content") else str(out).strip()

            # Determine hallucination: ensure at least one [source:] present
            hallucinated = "[source:" not in answer
            return {"answer": answer, "docs": [d.metadata for d in docs], "hallucinated": hallucinated}

    return answer_query



if __name__ == "__main__":
    load_dotenv()
    api_key = os.getenv("GOOGLE_API_KEY")


    df = load_csv("viettel.csv")
    docs = df_to_documents(df, "viettel.csv")
    chunks = chunk_documents(docs)
    vect = build_vectorstore(chunks)

    qa = make_qa_chain(vect, df=df, model_name="gemini-2.0-flash", temperature=0.0, fetch_k=FETCH_K, top_k=TOP_K, confidence_threshold=CONFIDENCE_THRESHOLD)

    

    queries = [
        "ƒê·ªÉ ƒëƒÉng k√Ω g√≥i MXH120 th√¨ so·∫°n tin g·ª≠i 191 ƒë√∫ng kh√¥ng?",
        "N√™u c√∫ ph√°p ƒëƒÉng k√Ω c·ªßa g√≥i c∆∞·ªõc c√≥ gi√° 120000 VNƒê v√† c√≥ ∆∞u ƒë√£i mi·ªÖn ph√≠ data",
        "Li·ªát k√™ t·∫•t c·∫£ c√°c g√≥i c∆∞·ªõc c√≥ chu k·ª≥ l·ªõn h∆°n 30 ng√†y?",
        "B·∫°n bao nhi√™u tu·ªïi?"
    ]
    for q in queries:
        print("\n---\nQUESTION:", q)
        res = qa(q)
        
        print("ANSWER:", res["answer"])
        print("HALLUCINATED:", res["hallucinated"])
        print("SOURCES:", res["docs"])