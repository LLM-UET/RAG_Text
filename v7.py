import os
from typing import List, Dict
import csv, glob
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI

from sentence_transformers import CrossEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import re
from dotenv import load_dotenv

def load_service_csv(path: str):
    docs = []
    with open(path, newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            def get(k): return row.get(k, "").strip()
            code = get("MÃ£ dá»‹ch vá»¥")
            text = (
                f"GÃ³i cÆ°á»›c {code} ({get('Thá»i gian thanh toÃ¡n')}). "
                f"GiÃ¡: {get('GiÃ¡ (VNÄ)')}Ä‘ / {get('Chu ká»³ (ngÃ y)')} ngÃ y. "
                f"4G tá»‘c Ä‘á»™ cao/ngÃ y: {get('4G tá»‘c Ä‘á»™ cao/ngÃ y')}. "
                f"Tá»± Ä‘á»™ng gia háº¡n: {get('Tá»± Ä‘á»™ng gia háº¡n')}. "
                f"CÃº phÃ¡p Ä‘Äƒng kÃ½: {get('CÃº phÃ¡p Ä‘Äƒng kÃ½')}. "
            )
            if get('Chi tiáº¿t'): text += f"Chi tiáº¿t: {get('Chi tiáº¿t')}. "
            if get('Gá»i ná»™i máº¡ng'): text += f"Gá»i ná»™i máº¡ng: {get('Gá»i ná»™i máº¡ng')}. "
            if get('Gá»i ngoáº¡i máº¡ng'): text += f"Gá»i ngoáº¡i máº¡ng: {get('Gá»i ngoáº¡i máº¡ng')}. "
            docs.append(Document(page_content=text, metadata={"source": path, "service_code": code}))
    return docs

def chunk_documents(docs: List[Document], chunk_size=800, chunk_overlap=100):
    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap
    )
    return splitter.split_documents(docs)

class HybridEmbeddings:
    def __init__(self, semantic_model_name="sentence-transformers/all-mpnet-base-v2"):
        self.semantic_model = HuggingFaceEmbeddings(model_name=semantic_model_name)
        self.vectorizer = TfidfVectorizer(analyzer="word", ngram_range=(1, 2))

        self.vectorizer.fit(["dummy"])

    def fit_lexical(self, texts: List[str]):
        """Huáº¥n luyá»‡n vectorizer TF-IDF vá»›i toÃ n bá»™ corpus Ä‘á»ƒ dÃ¹ng cho lexical embedding"""
        self.vectorizer.fit(texts)

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        sem_emb = self.semantic_model.embed_documents(texts)
        lex_emb = self.vectorizer.transform(texts).toarray()
        if lex_emb.shape[1] < len(sem_emb[0]):
            pad_width = len(sem_emb[0]) - lex_emb.shape[1]
            lex_emb = np.pad(lex_emb, ((0, 0), (0, pad_width)))
        elif lex_emb.shape[1] > len(sem_emb[0]):
            lex_emb = lex_emb[:, :len(sem_emb[0])]

        return (np.array(sem_emb) + lex_emb).tolist()

    def embed_query(self, text: str) -> List[float]:
        sem_vec = self.semantic_model.embed_query(text)
        lex_vec = self.vectorizer.transform([text]).toarray()[0]
        if len(lex_vec) < len(sem_vec):
            lex_vec = np.pad(lex_vec, (0, len(sem_vec) - len(lex_vec)))
        elif len(lex_vec) > len(sem_vec):
            lex_vec = lex_vec[:len(sem_vec)]
        return (np.array(sem_vec) + lex_vec).tolist()

def build_vectorstore(docs: List[Document], persist_dir: str="chroma_db", use_openai=False, hybrid=True):
    if use_openai:
        emb = OpenAIEmbeddings()
    else:
        if hybrid:
            print(">>> Using Hybrid Embeddings (TF-IDF + all-mpnet-base-v2)")
            emb = HybridEmbeddings(semantic_model_name="sentence-transformers/all-mpnet-base-v2")            
            texts = [d.page_content for d in docs]
            emb.fit_lexical(texts)
        else:
            emb = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
 
    vect = Chroma.from_documents(documents=docs, embedding=emb, persist_directory=persist_dir)
    # vect.persist()
    return vect

def extract_service_code(q: str):
    m = re.search(r"\b[A-Z]{1,5}\d+[A-Z]?\b", q.upper())
    return m.group(0) if m else None

def extract_registration_number(q: str):
    m = re.search(r"g[iÃ­]i?\s*(?:mÃ£\s*)?(?:sá»‘\s*)?(\d{2,4})", q.lower())
    if m:
        return m.group(1)
    m2 = re.search(r"gá»­i\s+(\d{2,4})", q.lower())
    return m2.group(1) if m2 else None

def deduplicate_docs(docs: List[Document]) -> List[Document]:
    seen = set()
    uniq = []
    for d in docs:
        key = (d.metadata.get("source"), d.metadata.get("service_code"), d.page_content.strip())
        if key not in seen:
            seen.add(key)
            uniq.append(d)
    return uniq

def make_reranked_retriever(vectorstore, fetch_k=50, top_k=5):
    """
    1. Láº¥y trÆ°á»›c fetch_k candidates tá»« retriever gá»‘c
    2. Rerank báº±ng cross-encoder (máº¡nh hÆ¡n cosine similarity)
    3. Tráº£ vá» top_k tá»‘t nháº¥t
    """
    retriever = vectorstore.as_retriever(search_kwargs={"k": fetch_k})
    reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

    def retrieve(question: str) -> List[Document]:
        code = extract_service_code(question)
        reg_num = extract_registration_number(question)

        candidates = retriever.invoke(question)
        if not candidates:
            return []
        
        candidates = deduplicate_docs(candidates)
        
        if code:
            exact_docs = [d for d in candidates if d.metadata.get("service_code") == code]
            if exact_docs: 
                candidates = exact_docs
        pairs = [(question, d.page_content) for d in candidates]
        scores = reranker.predict(pairs)

        boosted = []
        for d, s in zip(candidates, scores):
            boost = 0.0
            pc = d.page_content.upper()
            if code and code in pc:
                boost += 2.0
            if reg_num and reg_num in d.page_content:
                boost += 1.5

            boosted.append((d,s+boost))

        ranked = sorted(boosted, key=lambda x: x[1], reverse=True)
        reranked_docs = [doc for doc, _ in ranked[:top_k]]

        seen_codes = set()
        final = []
        for d in reranked_docs:
            sc = d.metadata.get("service_code")
            if sc not in seen_codes:
                final.append(d)
                seen_codes.add(sc)

        return final

    return retrieve


# RAG_PROMPT = """
# Vai trÃ²: Báº¡n lÃ  má»™t trá»£ lÃ½ áº£o thÃ´ng minh cÃ³ kháº£ nÄƒng giáº£i Ä‘Ã¡p tháº¯c máº¯c cá»§a khÃ¡ch hÃ ng cho nhÃ  máº¡ng viá»…n thÃ´ng.

# Nhiá»‡m vá»¥: XÃ¡c Ä‘á»‹nh xem báº¡n cÃ³ thá»ƒ tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng mÃ  chá»‰ dá»±a theo kiáº¿n thá»©c Ä‘Ã£ cho hay khÃ´ng, báº±ng cÃ¡ch truy váº¥n tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u.
# Báº¡n Ä‘Æ°á»£c cung cáº¥p má»™t cÆ¡ sá»Ÿ dá»¯ liá»‡u cÃ¡c gÃ³i cÆ°á»›c cá»§a Viettel. ÄÃ¢y lÃ  cÆ¡ sá»Ÿ dá»¯ liá»‡u dáº¡ng báº£ng, má»—i hÃ ng chá»©a thÃ´ng tin cá»§a má»™t gÃ³i, má»—i cá»™t chá»©a thuá»™c tÃ­nh cá»¥ thá»ƒ cá»§a gÃ³i Ä‘Ã³.
# Má»™t sá»‘ hÃ ng cÃ³ thá»ƒ trá»‘ng (optional).
# CÃ¡c cá»™t cá»§a báº£ng bao gá»“m: MÃ£ dá»‹ch vá»¥,Thá»i gian thanh toÃ¡n,CÃ¡c dá»‹ch vá»¥ tiÃªn quyáº¿t,GiÃ¡ (VNÄ),Chu ká»³ (ngÃ y),4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/ngÃ y,4G tá»‘c Ä‘á»™ cao/ngÃ y,4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/chu ká»³,4G tá»‘c Ä‘á»™ cao/chu ká»³,Gá»i ná»™i máº¡ng,Gá»i ngoáº¡i máº¡ng,Tin nháº¯n,Chi tiáº¿t,Tá»± Ä‘á»™ng gia háº¡n,CÃº phÃ¡p Ä‘Äƒng kÃ½

# ChÃº Ã½ 1: Náº¿u dá»¯ liá»‡u theo ngÃ y lÃ  sá»‘ dÆ°Æ¡ng thÃ¬ nghÄ©a lÃ  má»™t ngÃ y ngÆ°á»i dÃ¹ng chá»‰ Ä‘Æ°á»£c dÃ¹ng tá»‘i Ä‘a báº¥y nhiÃªu dá»¯ liá»‡u mÃ  thÃ´i, sang ngÃ y khÃ¡c láº¡i Ä‘Æ°á»£c thÃªm. CÃ²n náº¿u khÃ´ng cÃ³ dá»¯ liá»‡u theo ngÃ y thÃ¬ nghÄ©a lÃ  ngÆ°á»i dÃ¹ng Ä‘Æ°á»£c dÃ¹ng thoáº£i mÃ¡i toÃ n bá»™ dá»¯ liá»‡u trong chu ká»³ mÃ  khÃ´ng bá»‹ giá»›i háº¡n theo ngÃ y, cho Ä‘áº¿n khi háº¿t dá»¯ liá»‡u trong chu ká»³ Ä‘Ã³ thÃ¬ pháº£i chá» chu ká»³ tiáº¿p theo (náº¿u gia háº¡n) má»›i Ä‘Æ°á»£c tiáº¿p tá»¥c sá»­ dá»¥ng.
# ChÃº Ã½ 1b: Náº¿u ngÆ°á»i dÃ¹ng há»i dung lÆ°á»£ng thÃ¬ cáº§n chá»n cÃ¡c cá»™t sau: "4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/ngÃ y", "4G tá»‘c Ä‘á»™ cao/ngÃ y", "4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/chu ká»³", "4G tá»‘c Ä‘á»™ cao/chu ká»³", "Chi tiáº¿t".
# ChÃº Ã½ 2: Báº¡n pháº£i luÃ´n truy váº¥n cÃ¡c cá»™t sau trong má»i trÆ°á»ng há»£p: "MÃ£ dá»‹ch vá»¥", "CÃº phÃ¡p", "GiÃ¡ (VNÄ)" vÃ  "Chi tiáº¿t".
# ChÃº Ã½ 3: Náº¿u ngÆ°á»i dÃ¹ng nhá» tÆ° váº¥n cho Ä‘iá»‡n thoáº¡i cá»¥c gáº¡ch, nghe gá»i Ã­t hoáº·c Ã­t sá»­ dá»¥ng máº¡ng... thÃ¬ báº¡n cáº§n hiá»ƒu lÃ  pháº£i tÃ¬m gÃ³i cÆ°á»›c ráº» nháº¥t.
# ChÃº Ã½ 4: Náº¿u ngÆ°á»i dÃ¹ng há»i gÃ³i nÃ o ráº» hÆ¡n, thÃ¬ chá»‰ cáº§n so sÃ¡nh cá»™t "GiÃ¡ (VNÄ)
# ChÃº Ã½ 5: Náº¿u ngÆ°á»i dÃ¹ng há»i gÃ³i nÃ o ráº» nháº¥t, thÃ¬ cáº§n láº¥y giÃ¡ trá»‹ MIN cá»§a cá»™t "GiÃ¡ (VNÄ)"

# Náº¿u khÃ´ng tÃ¬m tháº¥y cÃ¢u tráº£ lá»i rÃµ rÃ ng trong cÆ¡ sá»Ÿ dá»¯ liá»‡u, hÃ£y tráº£ lá»i chÃ­nh xÃ¡c:
# "TÃ´i khÃ´ng biáº¿t - vui lÃ²ng liÃªn há»‡ tá»•ng Ä‘Ã i 18001090 hoáº·c email support@telco.vn"


# HÃ£y tráº£ lá»i:
# 1. Ngáº¯n gá»n, rÃµ rÃ ng, dá»… hiá»ƒu cho ngÆ°á»i dÃ¹ng phá»• thÃ´ng.
# 2. Giá»¯ nguyÃªn ngÃ´n ngá»¯ cá»§a ngÆ°á»i dÃ¹ng (Æ°u tiÃªn tiáº¿ng Viá»‡t).
# 3. Khi trÃ­ch dáº«n thÃ´ng tin, **luÃ´n ghi rÃµ nguá»“n theo dáº¡ng [source: filename]**.
# 4. KhÃ´ng suy luáº­n hoáº·c bá»‹a thÃ´ng tin khÃ´ng cÃ³ trong cÆ¡ sá»Ÿ dá»¯ liá»‡u.
# 5. Náº¿u tÃ¬m tháº¥y nhiá»u báº£n ghi trÃ¹ng nhau, chá»‰ cáº§n tÃ³m gá»n ná»™i dung chÃ­nh 1 láº§n.

# ---

# Ngá»¯ cáº£nh:
# {context}

# CÃ¢u há»i ngÆ°á»i dÃ¹ng:
# {question}

# Tráº£ lá»i:
# """

RAG_PROMPT = """
ğŸ¯ Vai trÃ²:
Báº¡n lÃ  má»™t **trá»£ lÃ½ áº£o thÃ´ng minh** cÃ³ nhiá»‡m vá»¥ há»— trá»£ khÃ¡ch hÃ ng vá» **cÃ¡c gÃ³i cÆ°á»›c cá»§a nhÃ  máº¡ng Viettel**.

---

ğŸ§© Nhiá»‡m vá»¥:
Báº¡n cáº§n **tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng chá»‰ dá»±a trÃªn dá»¯ liá»‡u Ä‘Ã£ cung cáº¥p trong "Ngá»¯ cáº£nh"**.  
Ngá»¯ cáº£nh lÃ  cÆ¡ sá»Ÿ dá»¯ liá»‡u dáº¡ng báº£ng, má»—i hÃ ng tÆ°Æ¡ng á»©ng vá»›i má»™t gÃ³i cÆ°á»›c, cÃ¡c cá»™t mÃ´ táº£ thuá»™c tÃ­nh cá»¥ thá»ƒ.

CÃ¡c cá»™t cá»§a báº£ng bao gá»“m:
> MÃ£ dá»‹ch vá»¥, Thá»i gian thanh toÃ¡n, CÃ¡c dá»‹ch vá»¥ tiÃªn quyáº¿t, GiÃ¡ (VNÄ), Chu ká»³ (ngÃ y),
> 4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/ngÃ y, 4G tá»‘c Ä‘á»™ cao/ngÃ y, 4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/chu ká»³, 4G tá»‘c Ä‘á»™ cao/chu ká»³,
> Gá»i ná»™i máº¡ng, Gá»i ngoáº¡i máº¡ng, Tin nháº¯n, Chi tiáº¿t, Tá»± Ä‘á»™ng gia háº¡n, CÃº phÃ¡p Ä‘Äƒng kÃ½.

Má»™t sá»‘ Ã´ cÃ³ thá»ƒ trá»‘ng (tÃ¹y gÃ³i cÆ°á»›c).

---

ğŸ§  Ghi nhá»› quy táº¯c:
1. Náº¿u dá»¯ liá»‡u "4G tá»‘c Ä‘á»™ cao/ngÃ y" hoáº·c "4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/ngÃ y" lÃ  sá»‘ dÆ°Æ¡ng  
   â†’ NghÄ©a lÃ  ngÆ°á»i dÃ¹ng Ä‘Æ°á»£c sá»­ dá»¥ng tá»‘i Ä‘a lÆ°á»£ng dá»¯ liá»‡u Ä‘Ã³ má»—i ngÃ y, sau Ä‘Ã³ reset vÃ o ngÃ y tiáº¿p theo.

2. Náº¿u **khÃ´ng cÃ³ dá»¯ liá»‡u theo ngÃ y**, nhÆ°ng cÃ³ dá»¯ liá»‡u theo chu ká»³  
   â†’ NghÄ©a lÃ  toÃ n bá»™ dung lÆ°á»£ng Ä‘Ã³ dÃ¹ng chung cho toÃ n chu ká»³.

3. Khi ngÆ°á»i dÃ¹ng há»i vá» **dung lÆ°á»£ng data**, hÃ£y tra cá»©u cÃ¡c cá»™t sau:
   - "4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/ngÃ y"
   - "4G tá»‘c Ä‘á»™ cao/ngÃ y"
   - "4G tá»‘c Ä‘á»™ tiÃªu chuáº©n/chu ká»³"
   - "4G tá»‘c Ä‘á»™ cao/chu ká»³"
   - "Chi tiáº¿t"

4. Trong **má»i cÃ¢u tráº£ lá»i**, báº¡n pháº£i trÃ­ch dáº«n tá»‘i thiá»ƒu cÃ¡c cá»™t:
   - "MÃ£ dá»‹ch vá»¥"
   - "CÃº phÃ¡p Ä‘Äƒng kÃ½"
   - "GiÃ¡ (VNÄ)"
   - "Chi tiáº¿t"

5. Náº¿u ngÆ°á»i dÃ¹ng nÃ³i â€œÄ‘iá»‡n thoáº¡i cá»¥c gáº¡châ€, â€œnghe gá»i Ã­tâ€, hoáº·c â€œÃ­t dÃ¹ng máº¡ngâ€  
   â†’ Hiá»ƒu lÃ  cáº§n **gá»£i Ã½ gÃ³i cÆ°á»›c ráº» nháº¥t** (tra cá»™t â€œGiÃ¡ (VNÄ)â€ Ä‘á»ƒ chá»n giÃ¡ nhá» nháº¥t).

6. Náº¿u ngÆ°á»i dÃ¹ng há»i **gÃ³i nÃ o ráº» hÆ¡n**,  
   â†’ So sÃ¡nh cá»™t â€œGiÃ¡ (VNÄ)â€ giá»¯a cÃ¡c gÃ³i.

7. Náº¿u ngÆ°á»i dÃ¹ng há»i **gÃ³i nÃ o ráº» nháº¥t**,  
   â†’ Chá»n gÃ³i cÃ³ giÃ¡ trá»‹ **MIN cá»§a cá»™t â€œGiÃ¡ (VNÄ)â€**.

8. Náº¿u cÃ³ **nhiá»u báº£n ghi trÃ¹ng láº·p**,  
   â†’ Chá»‰ cáº§n tá»•ng há»£p vÃ  **tráº£ lá»i tÃ³m táº¯t ná»™i dung chÃ­nh má»™t láº§n**.

---

ğŸš« Náº¿u khÃ´ng tÃ¬m tháº¥y cÃ¢u tráº£ lá»i rÃµ rÃ ng trong cÆ¡ sá»Ÿ dá»¯ liá»‡u, **hÃ£y tráº£ lá»i chÃ­nh xÃ¡c**:
"TÃ´i khÃ´ng biáº¿t - vui lÃ²ng liÃªn há»‡ tá»•ng Ä‘Ã i 18001090 hoáº·c email support@telco.vn"

---

ğŸ—£ï¸ YÃªu cáº§u Ä‘á»‹nh dáº¡ng cÃ¢u tráº£ lá»i:
- Viáº¿t **ngáº¯n gá»n, dá»… hiá»ƒu cho ngÆ°á»i dÃ¹ng phá»• thÃ´ng**.
- Giá»¯ nguyÃªn **ngÃ´n ngá»¯ cá»§a ngÆ°á»i dÃ¹ng** (Æ°u tiÃªn tiáº¿ng Viá»‡t).
- Khi trÃ­ch dáº«n, **luÃ´n ghi rÃµ nguá»“n theo dáº¡ng [source: filename]**.
- **Tuyá»‡t Ä‘á»‘i khÃ´ng suy luáº­n hoáº·c bá»‹a thÃ´ng tin** khÃ´ng cÃ³ trong dá»¯ liá»‡u.

---

Ngá»¯ cáº£nh:
{context}

CÃ¢u há»i ngÆ°á»i dÃ¹ng:
{question}

Tráº£ lá»i:
"""

prompt_template = ChatPromptTemplate.from_template(RAG_PROMPT)

def make_qa_chain(vectorstore, model_name="gemini-2.0-flash", temperature=0.0, use_openai_llm=True):
    if use_openai_llm:
        llm = ChatGoogleGenerativeAI(model=model_name, temperature=temperature)
    else:
        raise NotImplementedError("Only OpenAI LLM supported here")

    retriever_fn = make_reranked_retriever(vectorstore)

    def answer_query(question: str, k:int=5):
        docs = retriever_fn(question)

        context_pieces = []
        for d in docs:
            text = d.page_content.strip()
            context_pieces.append(f"[source: {os.path.basename(d.metadata.get('source',''))}] {text}")

        context = "\n\n".join(context_pieces) if context_pieces else ""
        prompt = RAG_PROMPT.format(context=context, question=question)
        resp = llm.invoke(prompt)
        answer = resp.content.strip() if hasattr(resp, "content") else str(resp)

        code = extract_service_code(question)
        reg_num = extract_registration_number(question)
        hallucinated = "[source:" not in answer or (code and code not in answer and not any(code in d.page_content for d in docs))
        return {"answer": answer, "docs": docs, "hallucinated": hallucinated}

    return answer_query


if __name__ == "__main__":
    load_dotenv()
    api_key = os.getenv("GOOGLE_API_KEY")
    docs = load_service_csv("viettel.csv")
    # print(docs)
    vect = build_vectorstore(docs, hybrid=True)
    print(f"VECTOR: {vect}")
    qa = make_qa_chain(vect)
    print(f"QA: {qa}")

    q_array = ["GÃ³i cÆ°á»›c SD70 cÃ³ giÃ¡ bao nhiÃªu vÃ  cung cáº¥p bao nhiÃªu GB data tá»‘c Ä‘á»™ tiÃªu chuáº©n trong má»™t chu ká»³ 30 ngÃ y?", "HÃ£y so sÃ¡nh gÃ³i cÆ°á»›c V90B vÃ  V120B. Sá»± khÃ¡c biá»‡t vá» giÃ¡, data tá»‘c Ä‘á»™ cao (tÃ­nh theo tá»•ng chu ká»³) vÃ  phÃºt gá»i ngoáº¡i máº¡ng lÃ  gÃ¬?", "Liá»‡t kÃª táº¥t cáº£ cÃ¡c gÃ³i cÆ°á»›c cÃ³ chu ká»³ lá»›n hÆ¡n 30 ngÃ y (tá»©c lÃ  gÃ³i dÃ i háº¡n) VÃ€ cÃ³ Æ°u Ä‘Ã£i miá»…n phÃ­ gá»i ná»™i máº¡ng (cá»¥ thá»ƒ lÃ  Miá»…n phÃ­ cÃ¡c cuá»™c gá»i dÆ°á»›i 10 phÃºt hoáº·c tÆ°Æ¡ng Ä‘Æ°Æ¡ng) HOáº¶C Æ°u Ä‘Ã£i data tá»‘c Ä‘á»™ cao theo ngÃ y lÃ  1GB? Náº¿u khÃ´ng cÃ³ gÃ³i nÃ o, hÃ£y giáº£i thÃ­ch táº¡i sao.", "Má»™t ngÆ°á»i dÃ¹ng sá»­ dá»¥ng gÃ³i 12MXH100. Giáº£ sá»­ giÃ¡ cÆ°á»›c khÃ´ng Ä‘á»•i, náº¿u há» dÃ¹ng gÃ³i cÆ°á»›c ngáº¯n háº¡n MXH100 trong cÃ¹ng 360 ngÃ y Ä‘Ã³, há» sáº½ pháº£i tráº£ thÃªm/bá»›t bao nhiÃªu tiá»n?", "GÃ³i cÆ°á»›c nÃ o cÃ³ Æ°u Ä‘Ã£i Ä‘áº·c biá»‡t lÃ  miá»…n phÃ­ tháº£ ga truy cáº­p khÃ´ng giá»›i háº¡n vÃ  nhá»¯ng máº¡ng xÃ£ há»™i nÃ o Ä‘Æ°á»£c bao gá»“m trong Æ°u Ä‘Ã£i nÃ y? NÃªu cÃº phÃ¡p Ä‘Äƒng kÃ½ cá»§a gÃ³i cÆ°á»›c cÃ³ giÃ¡ 120.000 VNÄ cÃ³ Æ°u Ä‘Ã£i nÃ y.", "CÃ³ bao nhiÃªu gÃ³i cÆ°á»›c trong báº£ng khÃ´ng cÃ³ thÃ´ng tin chi tiáº¿t vá» cuá»™c gá»i ná»™i máº¡ng (nghÄ©a lÃ  cá»™t Gá»i ná»™i máº¡ng bá»‹ bá» trá»‘ng), vÃ  chÃºng lÃ  nhá»¯ng gÃ³i nÃ o?"]
    
    for i, q in enumerate(q_array):
        result = qa(q)
        print(f"| Index: {i},| Question: {q} , | Result: {result}")
        print("HALLUCINATED:", result["hallucinated"])
        print("SOURCES:", [d.metadata for d in result["docs"]])

    # q = "Äá»ƒ Ä‘Äƒng kÃ½ gÃ³i MXH120 thÃ¬ soáº¡n tin gá»­i 191 Ä‘Ãºng khÃ´ng?"
    # result = qa(q)

    # print("ANSWER:", result["answer"])
    # print("HALLUCINATED:", result["hallucinated"])
    # print("SOURCES:", [d.metadata for d in result["docs"]])